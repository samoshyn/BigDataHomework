# -*- coding: utf-8 -*-
"""Coursework first try.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18hLsr80RTvmZWXxzIMrePSY7UB4ouJu3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from datetime import datetime
import re
from folium.plugins import HeatMap
import community
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import tqdm
from random import sample 
 
seed = 47

routes19_20 = pd.read_csv('/content/drive/MyDrive/Big Data Coursework/user_routes_2019_2020.csv.gz', compression='gzip', sep=',')
districts = pd.read_csv('/content/drive/MyDrive/Big Data Coursework/districts_info.csv.gz', compression='gzip', sep='|')

"""# Task 1: Clustering

На основании набора данных по одним и тем же абонентам за период 04.2019-09.2019 необходимо провести анализ и применить алгоритмы машинного обучения без учителя с целью выделения хорошо отличающихся сегментов абонентов. Также у вас есть данные по этим же абонентам и за "карантинный" период 04.2020-09.2020, чтобы вы могли проанализировать изменения в мобильности абонентов, вызванные ограничениями, введенными государством для противодействия распространению эпидемии. В итоге мы ожидаем получить сравнение показателей мобильности за периоды 2019 и 2020 для каждого из полученных сегментов.

### Feature Engineering
"""

# Конвертируем в дейттайм
# Добавляем столбцы только с датами для будущих агрегаций по календарным дням
# Сортируем для удобства
 
def make_datetime(df):
 
  df['start_time']= pd.to_datetime(df['start_time'])
  df['finish_time']= pd.to_datetime(df['finish_time'])
  df['date'] = df['start_time'].apply(lambda x: x.strftime("%Y-%m-%d"))
  df['date'] = pd.to_datetime(df['date'])
  df = df.sort_values(by='start_time').reset_index(drop=True)
 
  return df

routes19_20 = make_datetime(routes19_20)
 
# transactions_poltava['event_dt'] = pd.to_datetime(transactions_poltava['event_dt'])

# Фичи с дейттайма: месяц, час старта, час финиша, день недели, выходной или нет, длительность мува в часах
 
def make_datetime_features(df):
 
  df['month'] = df.start_time.dt.month
  df['hour_start'] = df.start_time.dt.hour
  df['hour_finish'] = df.finish_time.dt.hour
  df['weekday'] = df.start_time.dt.dayofweek
  df['dayoff'] = df['weekday'].apply(lambda x: 0 if x<5 else 1)
  df['duration_move'] = df['finish_time'] - df['start_time']
  df['duration_move'] = df.duration_move.dt.seconds / 3600
  del df['hmonth']
 
  return df

routes19_20 = make_datetime_features(routes19_20)

# Джоиним к сетам информацию со справочника по айдишникам локации старта и финиша
 
def join_district(df, dist):
 
  df = df.join(dist.set_index('area_id'), on='start_area_id', rsuffix='start').join(dist.set_index('area_id'), on='finish_area_id', rsuffix='_finish')
  df = df.dropna()
 
  return df

routes19_20 = join_district(routes19_20, districts)

#  Кропаем сеты по годам для сравнения
 
routes_19 = routes19_20[routes19_20.start_time.dt.year<2020]
routes_20 = routes19_20[routes19_20.start_time.dt.year>=2020]

"""## Statistics"""

def haversine_np(lon1, lat1, lon2, lat2):
  
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2
    c = 2 * np.arcsin(np.sqrt(a))
    km = 6367 * c
 
    return km

def adv_features(df):
 
  df = df.sort_values(['user_id','start_time']).reset_index(drop=True)
  df['same_region'] = (df['region_name']==df['region_name_finish']).astype(int)
  df['time_between_moves'] = (df['start_time'].shift(-1) - df['finish_time']) / np.timedelta64(1, 'h')
  df['time_between_moves'] = df['time_between_moves'].apply(lambda x: 0 if x<0 else x)
  df['haversine'] = haversine_np(df['centroid_lon'], df['centroid_lat'], df['centroid_lon_finish'], df['centroid_lat_finish'])
  df['part_time_work'] = ((df.dayoff==0) & (df.hour_start<=18) & (df.time_between_moves<6) & (df.time_between_moves>=3)).astype(int)
  df['full_time_work'] = ((df.dayoff==0) & (df.hour_finish>=8) & (df.hour_start<=18) & (df.time_between_moves>=6) & (df.time_between_moves<=15)).astype(int)
 
  return df

routes_19 = adv_features(routes_19)
routes_20 = adv_features(routes_20)

r19 = routes_19.groupby(['date'])['user_id'].count().to_frame().reset_index()
r19['index'] = [i+1 for i in range(len(r19))]
r19['travel_count'] = r19['user_id']
r19['count_rolling'] = r19['travel_count'].rolling(7).mean()
r20 = routes_20.groupby(['date'])['user_id'].count().to_frame().reset_index()
r20['index'] = [i+1 for i in range(len(r20))]
r20['travel_count'] = r20['user_id']
r20['count_rolling'] = r20['travel_count'].rolling(7).mean()

fig = plt.figure(figsize=(6,6))
sns.lineplot(x='index', y='count_rolling' ,data=r19, label='2019 рік')
sns.lineplot(x='index', y='count_rolling' ,data=r20, label='2020 рік')
plt.text(15, 23000, 'локдаун', fontsize=14, color='r')
plt.text(80, 12000, 'зональний карантин', fontsize=14, color='tab:orange')
plt.plot([70, 70], [10000, 25000], color='r', linewidth=2)
#locs, labels = plt.xticks()
plt.xticks([5, 35, 65, 95, 125, 155, 180], [4, 5, 6, 7, 8, 9, 10])
plt.xlabel('month')
plt.legend()
plt.title('Порівняння кількості переміщеннь')
plt.savefig('picture 1.png')

def select_home(df):
 
  finish = df[(df.hour_finish>=20)].groupby('user_id')['finish_area_id'].value_counts().to_frame().rename(columns={'finish_area_id':'count_stay_finish'}).reset_index()
  start = df[(df.hour_start<=10)].groupby('user_id')['start_area_id'].value_counts().to_frame().rename(columns={'start_area_id':'count_stay_start'}).reset_index()
 
  all_place_by_user = pd.merge(start.rename(columns={'start_area_id':'area_id'}), finish.rename(columns={'finish_area_id':'area_id'}), on=['user_id', 'area_id'])
  all_place_by_user['count'] = all_place_by_user['count_stay_start'] + all_place_by_user['count_stay_finish']
  all_place_by_user = all_place_by_user.sort_values(by=['user_id', 'count'], ascending=False).drop_duplicates(subset='user_id', keep='first')[['user_id', 'area_id']]
  
  statictics = df[df.month<=5].groupby(['user_id'])['start_area_id'].count().reset_index().rename(columns={'start_area_id':'count_move_4_5'})
  statictics['count_move_6_9'] = df[df.month>5].groupby(['user_id'])['start_area_id'].count().reset_index().rename(columns={'start_area_id':'count_move_6_9'})['count_move_6_9']
  statictics = statictics.join(all_place_by_user[['user_id', 'area_id']].rename(columns={'area_id': 'home_id'}).set_index('user_id'), on='user_id')
  for user in statictics[statictics.home_id.isna()==True].user_id.unique():
    maybe_home = df[df.user_id==user].start_area_id.value_counts().index[0]
    statictics.loc[statictics.user_id==user, 'home_id'] = maybe_home
 
  df = df.join(statictics[['user_id', 'home_id']].set_index('user_id'), on='user_id')
  df['in_home'] = (df['finish_area_id']==df['home_id']).astype(int)
 
  return df, statictics

routes_19_full, stat_19 = select_home(routes_19)
routes_20_full, stat_20 = select_home(routes_20)

def agg_stat(df, stat):
  new_stat = df.groupby(['user_id']).agg({'same_region': ['mean'],  'full_time_work': ['mean'], 'part_time_work': ['mean'], 'haversine': ['mean'], 'time_between_moves': ['mean']})
  new_stat.columns = ['_'.join(col).strip() for col in new_stat.columns.values]
  new_stat.reset_index(inplace=True)
  stat = pd.merge(stat, new_stat, on='user_id', how='left')
 
  new_stat1 = df.groupby(['user_id', 'date']).agg({'start_area_id': ['count']}).groupby('user_id').mean()
  new_stat1.columns = ['_'.join(col).strip() for col in new_stat1.columns.values]
  new_stat1.reset_index(inplace=True)
  stat = pd.merge(stat, new_stat1, on='user_id', how='left')
 
  home_stat1 = df[df.in_home==1].groupby('user_id').agg({'haversine': ['mean'], 'time_between_moves': ['mean']})
  home_stat1.columns = ['_inhome_'.join(col).strip() for col in home_stat1.columns.values]
  home_stat1.reset_index(inplace=True)
  stat = pd.merge(stat, home_stat1, on='user_id', how='left')
 
  home_stat2 = df[df.in_home==1].groupby(['user_id']).agg({'time_between_moves': ['sum']}) / (24* df.date.nunique())
  home_stat2.columns = ['_inhome_'.join(col).strip() for col in home_stat2.columns.values]
  home_stat2.reset_index(inplace=True)
  stat = pd.merge(stat, home_stat2, on='user_id', how='left')

  home_stat1 = df[df.in_home==0].groupby('user_id').agg({'haversine': ['mean'], 'time_between_moves': ['mean']})
  home_stat1.columns = ['_away_'.join(col).strip() for col in home_stat1.columns.values]
  home_stat1.reset_index(inplace=True)
  stat = pd.merge(stat, home_stat1, on='user_id', how='left')
 
  home_stat2 = df[df.in_home==0].groupby(['user_id']).agg({'time_between_moves': ['sum']}) / (24* df.date.nunique())
  home_stat2.columns = ['_away_'.join(col).strip() for col in home_stat2.columns.values]
  home_stat2.reset_index(inplace=True)
  stat = pd.merge(stat, home_stat2, on='user_id', how='left')

  stat = stat.fillna(stat.mean())
  del stat['home_id']
 
  return stat

stat_19_temp = stat_19.copy()
stat_20_temp = stat_20.copy()

stat_20_4_5 = agg_stat(routes_20_full[routes_20_full.month<=5], stat_20_temp)

stat_19_temp = stat_19.copy()
stat_20_temp = stat_20.copy()

stat_19_all = agg_stat(routes_19_full, stat_19_temp)
stat_20_all = agg_stat(routes_20_full[routes_20_full.month>5], stat_20_temp)

def create_rating(df):

  df['mobility_rating'] = (df['haversine_inhome_mean'] * df['same_region_mean'] + df['haversine_away_mean'] * (1-df['same_region_mean'])) * \
  ((1-df['full_time_work_mean']) + (1-df['part_time_work_mean']))

  outlier_user = df[df['mobility_rating']>200]
  outlier_user['mobility_rating'] = 10
  outlier_user_list = outlier_user.user_id.unique().tolist()

  norm_df = df[~df['user_id'].isin(outlier_user_list)]
  norm_df['mobility_rating'] = (norm_df['mobility_rating']-norm_df['mobility_rating'].min())/(norm_df['mobility_rating'].max()-norm_df['mobility_rating'].min())
  norm_df['mobility_rating'] = 1 + 9 * norm_df['mobility_rating']

  df = pd.concat([norm_df, outlier_user])
  df = df.sort_values(['user_id']).reset_index(drop=True)

  return df

# fig, ax = plt.subplots(3,1,figsize=(7,7))
# sns.histplot(stat_19_all['mobility_rating'], label='2019 рік', ax=ax[0], color='g', bins=10)
# ax[0].legend(loc=1)
# ax[0].set_title('Розподіл визначеного рівня мобільності')
# sns.histplot(stat_20_4_5['mobility_rating'], label='2020 рік, локдаун', ax=ax[1], color='r', bins=10)
# ax[1].legend(loc=1)
# sns.histplot(stat_20_all['mobility_rating'], label='2020 рік, зональний карантин', ax=ax[2], color='y', bins=10)
# ax[2].legend(loc=1)
# plt.savefig('picture 2.png')

stat_20_4_5 = create_rating(stat_20_4_5)

stat_19_all = create_rating(stat_19_all)
stat_20_all = create_rating(stat_20_all)

stat_19_all['count_move'] = stat_19_all['count_move_4_5'] + stat_19_all['count_move_6_9']
stat_20_all['count_move'] = stat_20_all['count_move_4_5'] + stat_20_all['count_move_6_9']
stat_20_4_5['count_move'] = stat_20_4_5['count_move_4_5'] + stat_20_4_5['count_move_6_9']

"""## Make KMeans clusters"""

X20_4_5 = stat_20_4_5[['mobility_rating', 'count_move']].values

X19_all = stat_19_all[['mobility_rating', 'count_move']].values
X20_all = stat_20_all[['mobility_rating', 'count_move']].values

from sklearn.preprocessing import MinMaxScaler, StandardScaler

X20_4_5_scaled = StandardScaler().fit_transform(X20_4_5)

X19_all_scaled = StandardScaler().fit_transform(X19_all)
X20_all_scaled = StandardScaler().fit_transform(X20_all)

from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_samples, silhouette_score

wcss_4_5 = []
for i in range(2, 7):
    kmeans = KMeans(n_clusters = i, random_state = seed)
    cluster_labels = kmeans.fit_predict(X19_all_scaled)
    wcss_4_5.append(kmeans.inertia_)
    silhouette_avg = silhouette_score(X19_all_scaled, cluster_labels)
    print("For n_clusters =", i, "in 4-5 month. The average silhouette_score is :", silhouette_avg)

sns.lineplot(range(2, 7), wcss_4_5,marker='o',color='red')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

def image_clusters(X19, y19, X20, y20, kmeansmodel, crop=1):

  fig = plt.figure(figsize=(8,8))
  ax = fig.add_subplot(121)
  if crop==1:
    name = '4-5 місяць'
  if crop==0:
    name = '6-9 місяць'
  if crop==2:
    name = 'Полтава'
  ax.set_ylim(0, 7)
  values_19 = pd.value_counts(y19, normalize=True).values
  ax.scatter(X19[y19 == 0, 0], X19[y19 == 0, 1], c = 'red', label = f'Cluster 1 ({np.round(values_19[0], 2)})')
  ax.scatter(X19[y19 == 1, 0], X19[y19 == 1, 1], c = 'blue', label = f'Cluster 2 ({np.round(values_19[1], 2)})')
  ax.scatter(X19[y19 == 2, 0], X19[y19 == 2, 1], c = 'green', label = f'Cluster 3 ({np.round(values_19[2], 2)})')
  #ax.scatter(kmeansmodel.cluster_centers_[:, 0], kmeansmodel.cluster_centers_[:, 1], s = 150, c = 'yellow', label = 'Centroids')
  plt.title(f'Кластери користувачів у 2019')
  plt.xlabel('mobility_rating')
  plt.ylabel('count_move')
  plt.legend(loc=1)
  

  ax = fig.add_subplot(122)
  ax.set_ylim(0, 7)
  values_20 = pd.value_counts(y20, normalize=True).values
  ax.scatter(X20[y20 == 0, 0], X20[y20 == 0, 1], c = 'red', label = f'Cluster 1 ({np.round(values_20[0], 2)})')
  ax.scatter(X20[y20 == 1, 0], X20[y20 == 1, 1], c = 'blue', label = f'Cluster 2 ({np.round(values_20[1], 2)})')
  ax.scatter(X20[y20 == 2, 0], X20[y20 == 2, 1], c = 'green', label =f'Cluster 3 ({np.round(values_20[2], 2)})')
  #ax.scatter(kmeansmodel.cluster_centers_[:, 0], kmeansmodel.cluster_centers_[:, 1], s = 150, c = 'yellow', label = 'Centroids')
  plt.title(f'Кластери користувачів у Полтаві')
  plt.xlabel('mobility_rating')
  plt.ylabel('count_move')
  plt.legend(loc=1)
  plt.savefig('picture 3.png')

def make_clusters(X19, X20, crop=1):

  kmeansmodel = KMeans(n_clusters=3, init='k-means++', random_state=seed)

  y_kmeans19 = kmeansmodel.fit_predict(X19)
  y_kmeans20 = kmeansmodel.predict(X20)

  image_clusters(X19, y_kmeans19, X20, y_kmeans20, kmeansmodel, crop)

  return y_kmeans19, y_kmeans20

from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_samples, silhouette_score

y_kmeans19_all, y_kmeans20_4_5 = make_clusters(X19_all_scaled, X20_4_5_scaled, 1)
y_kmeans19_all, y_kmeans20_all = make_clusters(X19_all_scaled, X20_all_scaled, 0)

"""## Make results and visualize"""

stat_20_4_5['cluster'] = y_kmeans20_4_5

stat_19_all['cluster'] = y_kmeans19_all
stat_20_all['cluster'] = y_kmeans20_all

def brief_stat(stat19, stat20, crop=1):
  if crop==1:
    name = '4-5 month'
  if crop==0:
    name = '6-9 month'
  if crop==2:
    name = 'Poltava'
  brief = pd.DataFrame([stat19['cluster'].value_counts(sort=False), stat19['cluster'].value_counts(1, sort=False)*100, 
              stat20['cluster'].value_counts(sort=False), stat20['cluster'].value_counts(1, sort=False)*100], 
             index=[f'Сount (2019)', f'% (2019)', f'Сount (2020 ({name}))', f'% (2020 ({name}))']).transpose()
  if crop!=2:
    brief['Diff Count'] = brief[f'Сount (2020 ({name}))'] - brief[f'Сount (2019)']
  brief['Diff %'] = brief[f'% (2020 ({name}))'] - brief[f'% (2019)']
  return brief

brief_stat(stat_19_all, stat_20_4_5, 1)

brief_stat(stat_19_all, stat_20_all, 0)

def make_groupby(stat19, stat20, feature, crop=1):
  if crop==1:
    name = '4-5 month'
  if crop==0:
    name = '6-9 month'
  if crop==2:
    name = 'Poltava'
  brief = pd.DataFrame([stat19.groupby('cluster')[feature].mean(), stat20.groupby('cluster')[feature].mean(),
                        stat19.groupby('cluster')[feature].std(), stat20.groupby('cluster')[feature].std()], 
                        index=[f'Mean {feature} (2019)', f'Mean {feature} (2020 ({name}))', f'Std {feature} (2019)', f'Std {feature} (2020 ({name}))']).transpose()
  brief['Diff Mean'] = brief[f'Mean {feature} (2020 ({name}))'] - brief[f'Mean {feature} (2019)']
  brief['Diff Std'] = brief[f'Std {feature} (2020 ({name}))'] - brief[f'Std {feature} (2019)']
  return brief

def make_boxplot(stat19, stat200, stat201, feature, ymax):

  fig = plt.figure(figsize=(12,6))

  ax = fig.add_subplot(131)
  sns.boxplot(x='cluster', y=feature, data=stat19, ax=ax)
  ax.set_title(f'By {feature} (2019)')
  ax.set_ylim(0, ymax)

  ax = fig.add_subplot(132)
  sns.boxplot(x='cluster', y=feature, data=stat200, ax=ax)
  ax.set_title(f'{feature} (2020 4-5 month)')
  ax.set_ylim(0, ymax)

  ax = fig.add_subplot(133)
  sns.boxplot(x='cluster', y=feature, data=stat201, ax=ax)
  ax.set_title(f'{feature} (2020 6-9 month)')
  ax.set_ylim(0, ymax)
  plt.savefig('boxplot.png')
  plt.show()

def make_bar(stat_19_all, stat_20_4_5, stat_20_all, check20all, feature):

  N = len(list(set(stat_19_all['cluster']) ))
  count_move_19 = [stat_19_all[stat_19_all['cluster'] == i][feature].mean() for i in list(set(stat_19_all['cluster']) )]
  count_move_20_4_5 = [stat_20_4_5[stat_20_4_5['cluster'] == i][feature].mean() for i in list(set(stat_20_4_5['cluster']) )]
  if check20all!=0:
    count_move_20_all = [stat_20_all[stat_20_all['cluster'] == i][feature].mean() for i in list(set(stat_20_all['cluster']) )]


  ind = np.arange(N) 
  fig, ax = plt.subplots(figsize = (8,6))
  width = 0.2       
  plt.bar(ind, count_move_19, width, label='2019')
  plt.bar(ind + width, count_move_20_4_5, width, label='2020 (lockdown)')
  if check20all!=0:
    plt.bar(ind + 2*width, count_move_20_all, width, label='2020 (6-9 month)')

  plt.ylabel(feature)
  plt.xlabel('Clusters')
  plt.title(feature)

  plt.xticks(ind , list(set(stat_19_all['cluster'])))
  plt.legend()
  plt.savefig('barplot.png')
  plt.show()

make_boxplot(stat_19_all, stat_20_4_5, stat_20_all, 'mobility_rating', 12)

make_groupby(stat_19_all, stat_20_4_5, 'mobility_rating', 1)

make_groupby(stat_19_all, stat_20_all, 'mobility_rating', 0)

make_bar(stat_19_all, stat_20_4_5, stat_20_all, 1, 'mobility_rating')

N = len(list(set(stat_19_all['cluster']) ))
count_move_191 = [stat_19_all[stat_19_all['cluster'] == i]['count_move_4_5'].mean() for i in list(set(stat_19_all['cluster']) )]
count_move_192 = [stat_19_all[stat_19_all['cluster'] == i]['count_move_6_9'].mean() for i in list(set(stat_19_all['cluster']) )]

count_move_201 = [stat_20_4_5[stat_20_4_5['cluster'] == i]['count_move_4_5'].mean() for i in list(set(stat_20_4_5['cluster']) )]
count_move_202 = [stat_20_4_5[stat_20_4_5['cluster'] == i]['count_move_6_9'].mean() for i in list(set(stat_20_4_5['cluster']) )]


ind = np.arange(N) 
fig, ax = plt.subplots(figsize = (6,6))
width = 0.2       
p1 = plt.bar(ind, count_move_191, width, label='2019 (4-5 month)', color='blue')
p2 = plt.bar(ind, count_move_192, width, bottom=count_move_191,  label='2019 (6-9 month)')
p3 = plt.bar(ind+width, count_move_201, width, label='2020 (4-5 month)')
p4 = plt.bar(ind+width, count_move_202, width, bottom=count_move_201,  label='2020 (6-9 month)', color='orange')
'''plt.bar(ind + width, count_move_20_4_5, width, label='2020 (lockdown)')
if check20all!=0:
  plt.bar(ind + 2*width, count_move_20_all, width, label='2020 (6-9 month)')'''

plt.ylabel('сount_move')
plt.xlabel('Clusters')
plt.title('Порівняння кількості поїздок')

plt.xticks(ind , list(set(stat_19_all['cluster'])))
plt.legend()
plt.savefig('barplot.png')
plt.show()

make_groupby(stat_19_all, stat_20_4_5, 'count_move', 1)

make_groupby(stat_19_all, stat_20_all, 'count_move', 0)

make_boxplot(stat_19_all, stat_20_4_5, stat_20_all, 'full_time_work_mean', 1)

make_groupby(stat_19_all, stat_20_4_5, 'full_time_work_mean', 1)

make_groupby(stat_19_all, stat_20_all, 'full_time_work_mean', 0)

make_bar(stat_19_all, stat_20_4_5, stat_20_all, 1, 'full_time_work_mean')

make_boxplot(stat_19_all, stat_20_4_5, stat_20_all, 'start_area_id_count', 6)

make_groupby(stat_19_all, stat_20_4_5, 'haversine_inhome_mean', 1)

make_groupby(stat_19_all, stat_20_all, 'haversine_inhome_mean', 0)

make_groupby(stat_19_all, stat_20_4_5, 'haversine_away_mean', 1)

make_groupby(stat_19_all, stat_20_all, 'haversine_away_mean', 0)

make_bar(stat_19_all, stat_20_4_5, stat_20_all, 1, 'haversine_inhome_mean')

make_bar(stat_19_all, stat_20_4_5, stat_20_all, 1, 'haversine_away_mean')

# sns.pairplot(stat_19_all[['mobility_rating', 'count_move', 'full_time_work_mean', 'haversine_away_mean', 'start_area_id_count', 'cluster']],hue='cluster',palette='Dark2',diag_kind='kde')

# sns.pairplot(stat_20_4_5[['mobility_rating', 'count_move', 'full_time_work_mean', 'haversine_away_mean', 'start_area_id_count', 'cluster']],hue='cluster',palette='Dark2',diag_kind='kde')

"""## What is done?



1.   Главная задача созданной сегментации оценить мобильность пользователей в разрезе 6 месяцев. Для этого используем разработанные ключевые показателей, их интерпретации и ограничения. Из важных показателей стоит отметить:

  *   **Count of moves:** сумма всех перемещений пользователя за отведенное время. Агрегируется до дня. **Его основное использование** - помочь в потенциальном масштабировании других индикаторов в случае внезапных изменений в шаблонах использования, которые, в свою очередь, могут повлиять на измерения других индикаторов.
  *   **Mean distance traveled (вне домашнего района):** среднее расстояние, пройденное каждым пользователем в другие (не домашние) районы за отведенный промежуток. Длина определяется как расстояние между координатами текущей локации и следующей транзакции. **Преимущества:** всязующее звено для ежедневного пройденного расстояния. Поскольку он отслеживает перемещение между разными локациями, он не только фиксирует движение по районам (как и все другие индикаторы мобильности), но также и движение вне домашней административной зоны. Следовательно, это может помочь лучше учесть степень мобильности. Во время карантина ожидается увидеть снижение этого индикатора и в свою очередь увеличения внутри района. **Недостатки:** низкая точность потому, что считаем расстояние между координатными центрами, а не фактическими координатами перемещения.
  *   **Unique location count mean:** аггрегирует среднее количество уникальных локаций, посещенных пользователем за день. **TODO**
  *   **Mobility rating:**  **TODO**


2.   Делаем кластеризацию. Почему эти параметры, как выбирали количество кластеров, какой метод кластеризации, зачем поделили на 2 промежутка. **TODO**

## Cluster profile (2019)

*   **Кластер 0** - найменьший кластер (9к пользователей, 18%) **Характеристика:** 
  1. средняя мобильность (3.2), но имеют много аутлаеров с большей мобильностью;
  2. екстремально большое количество поездок (в среднем 205), но с большим разборосом (в месяц)
  3. (по составленной гипотезе о работе) Порядка 25% поездок - рабочего характера на полную занятность.
  4. Если едут домой, то не из далека (80% соседних районов).
  5. Поездки, не связанные с домом аналогичные. 30-40км, скорее всего это в одной области.

**Итого:** типичные работяги, без особых внештатных поездок. Но из-за повышенного std получаем, что есть люди, которые умудряются и ездить далеко (в выходные дни, скорее всего на отдых).

*   **Кластер 1** - второй по популярности кластер (14к пользователей и 23% всех юзеров). **Характеристика:** 
  1. высокая мобильность (8.7);
  2. редко ездят (34)
  3. 5% поездок - рабочего характера на полную занятность (тут уже маловероятно, что это именно работа)
  4. Более чем в 3 раза продолжительнее дальня поездка домой, чем у других кластеров (117 км), учитывая карту украины, то это разные области. Но с большим разбросом.
  5. Также предпочитают дальние поездки вне дома (113 км).

**Итого:** скорее всего не работают в офисе, часто далеко ездят.

*   **Кластер 2** - самый большой кластер (порядка 29к пользователей или 59%). **Характеристика:** 
  1. средняя мобильность (3.7); 
  2. ездят чаще чем 1 кластер (50)
  3. 7% поездок - рабочего характера на полную занятность (тут уже маловероятно, что это именно работа)
  4. Аналогично кластеру 0.
  5. Поездки, не связанные с домом аналогичные. 40-50км, скорее всего это в одной области.

**Итого:** типичные работяги, без особых внештатных поездок даже по выходным.

## Cluster profile (2020)

*   **Кластер 0** 
  1. Явное снижение мобильности во время локдауна (т.е. маленькая активность, добавить что входит в оценку)
  2. Размер кластера почти неизменный.
  3. Меньше поездок (-5).
  4. Меньше поездок, после которых долго на одном месте (скорее всего потеря работы).
  5. Поездки, не связанные с домом аналогичные. 30-40км, скорее всего это в одной области.

*   **Кластер 1**
  1. снижение высокой мобильности почти на 1.5 балла при увеличении дисперсии (т.е. активность по дальним поездкам упала, но во время локдауна все равно были)
  2. Заметное снижение размера (на 10% во время локдауна, закрыли межобластное соединение).
  3. Длительные поездки ушли, пришли больше коротких (+7).
  4. Более чем в 3 раза продолжительнее дальня поездка домой, чем у других кластеров (117 км), учитывая карту украины, то это разные области. Но с большим разбросом.
  5. Также предпочитают дальние поездки вне дома (113 км).

*   **Кластер 2** 
  1. наиболее ответсвенные граждане в локдауне. 
  2. На 15 процентов стало больше.
  3. Меньше поездок (-10).
  4. Очень редкие поездки (дисперсия +10) в близь своего района.
  5. Почти на нет поездки в другие области.

**Общее после локдауна:**
  1. После локдауна мобильность вернулась на прежний уровень (связано с возобновление работы + транспорт)
  2. Всего лишь 2 процента из тех, кто начал уменьшать мобильность, продолжил после локдауна (наверно людям нужно работать).
  3. Количество вернулось на прежние позиции.
  4. Поездки на работу вернулись в прежнее русло. (-0.03 в среднем).
  5. После локдауна Кластер 2 все так же соблюдает карантин (воздерживается от далеких поездок).

# Task 2: Poltava

У вас есть данные всех транзакций абонентов за 1 месяц, которые фиксировались в одном из городов (у каждой команды свой город), а также данные по перемещениям этих же абонентов за 2019 год аналогично задаче №1. Необходимо выделить аналогичные сегменты для этой выборки абонентов и для каждого сегмента, на основании транзакционных данных, проанализировать каким образом они въезжали и выезжали из города.

## Task 2.1: Poltava clustering
"""

routes_poltava = pd.read_csv('/content/drive/MyDrive/Big Data Coursework/user_routes_Poltava.csv.gz', compression='gzip', sep=',')
transactions_poltava = pd.read_csv('/content/drive/MyDrive/Big Data Coursework/user_transactions_Poltava.csv.gz', compression='gzip', sep=',')

routes_poltava = make_datetime(routes_poltava)

routes_poltava = make_datetime_features(routes_poltava)

routes_poltava = join_district(routes_poltava, districts)

routes_poltava = adv_features(routes_poltava)

routes_poltava, stat = select_home(routes_poltava)

stat_potlava = agg_stat(routes_poltava, stat)

stat_potlava = create_rating(stat_potlava)

stat_potlava['count_move'] = stat_potlava['count_move_4_5'] + stat_potlava['count_move_6_9']

X_poltava = stat_potlava[['mobility_rating', 'count_move']].values
X_poltava_scaled = StandardScaler().fit_transform(X_poltava)

_, y_kmeans_poltava = make_clusters(X19_all_scaled, X_poltava_scaled, 1)
stat_potlava['cluster'] = y_kmeans_poltava

def make_city_boxplot(stat19, city, feature, ymax):

  fig = plt.figure(figsize=(8,4))

  ax = fig.add_subplot(121)
  sns.boxplot(x='cluster', y=feature, data=stat19, ax=ax)
  ax.set_title(f'By {feature} (2019)')
  ax.set_ylim(0, ymax)

  ax = fig.add_subplot(122)
  sns.boxplot(x='cluster', y=feature, data=city, ax=ax)
  ax.set_title(f'By {feature} (2019 (Poltava))')
  ax.set_ylim(0, ymax)
  plt.savefig('box_plot.png')
  plt.show()

def barplot_city(stat_19_all, stat_potlava, feature):
  N = len(list(set(stat_19_all['cluster']) ))
  count_move_19 = [stat_19_all[stat_19_all['cluster'] == i][feature].mean() for i in list(set(stat_19_all['cluster']) )]
  count_move_20_4_5 = [stat_potlava[stat_potlava['cluster'] == i][feature].mean() for i in list(set(stat_potlava['cluster']) )]
  comp = np.array(count_move_20_4_5) - np.array(count_move_19)

  ind = np.arange(N) 
  fig, ax = plt.subplots(figsize = (8,6))
  width = 0.2       
  plt.bar(ind, count_move_19, width, label='2019')
  plt.bar(ind + width, count_move_20_4_5, width, label='Poltava')
  plt.bar(ind + 2*width, comp, width, label='Difference')

  plt.ylabel(feature)
  plt.xlabel('Clusters')
  plt.title(f'{feature} difference in Poltava')

  plt.xticks(ind , list(set(stat_19_all['cluster'])))
  plt.legend()
  plt.savefig('barplot.png')
  plt.show()

brief_stat(stat_19_all, stat_potlava, 2)

make_city_boxplot(stat_19_all, stat_potlava, 'mobility_rating', 12)

barplot_city(stat_19_all, stat_potlava, 'mobility_rating')

make_city_boxplot(stat_19_all, stat_potlava, 'count_move', 800)

barplot_city(stat_19_all, stat_potlava, 'count_move')

make_city_boxplot(stat_19_all, stat_potlava, 'full_time_work_mean', 1)

stat_potlava

barplot_city(stat_19_all, stat_potlava, 'haversine_away_mean')

make_city_boxplot(stat_19_all, stat_potlava, 'start_area_id_count', 6)

make_bar(stat_19_all, stat_potlava, None, 0, 'start_area_id_count')

"""## Comparison of clusters

Из-за меньшего количество точек, метод кластеризации с теми же центроидами дает менее отчетливые кластера визуально. Без изменений остался Кластер 2 (ответвенные маломобильные юзеры). Остальные кластера получились не так сконцентрированые в центроиде.

Если сравнивать показатели самых важных индикаторов, то можно заметить, что ящики с усами остались почти без изменений (различия в размере +\- 5% к дисперсии и квантилям). Показатели, на которых базирутуются наши кластеры, остались без изменений.

Размеры кластеров не изменили на статистически значимую величину. (-5% кластер 2, +3.5% Кластер 1, +1.5% Кластер 0).

## Task 2.2 Points of entry and exit from the city
"""

!pip install geojson

import geojson
from ast import literal_eval  

with open('/content/drive/MyDrive/Big Data Coursework/Poltava.geojson','r') as data_file:
    geo = geojson.load(data_file)
    
geo = geo[:-12] + geo[-1]

geo_dict = literal_eval(geo)

coordinates = geo_dict['coordinates'][0]

longitude_poltava = []
latitude_poltava = []
for i in range(len(coordinates)):
    longitude_poltava.append(coordinates[i][0])
    latitude_poltava.append(coordinates[i][1])

min_lat, max_lat, min_lon, max_lon = np.min(latitude_poltava), np.max(latitude_poltava), np.min(longitude_poltava), np.max(longitude_poltava)

transactions_poltava['event_dt'] = pd.to_datetime(transactions_poltava['event_dt'])
transactions_poltava['event_dt'] = transactions_poltava['event_dt'].dt.to_period('1T')
transactions_poltava = transactions_poltava.groupby(['user_id', 'event_dt'])[['lat', 'lon']].median().reset_index()

#transactions_poltava['date'] = transactions_poltava['event_dt'].apply(lambda x: x.strftime("%Y-%m-%d"))
#transactions_poltava['date'] = pd.to_datetime(transactions_poltava['date'])
#transactions_poltava = transactions_poltava.sort_values(by='event_dt').reset_index(drop=True)

#transactions_poltava['month'] = transactions_poltava.event_dt.dt.month
#transactions_poltava['hour'] = transactions_poltava.event_dt.dt.hour
transactions_poltava['weekday'] = transactions_poltava.event_dt.dt.dayofweek
transactions_poltava['dayoff'] = transactions_poltava['weekday'].apply(lambda x: 0 if x<5 else 1)
transactions_poltava['in_poltava'] = ((transactions_poltava['lat']<=max_lat) & (transactions_poltava['lat']>=min_lat) & (transactions_poltava['lon']<=max_lon) & (transactions_poltava['lon']>=min_lon)).astype(int)

transactions_poltava.in_poltava.value_counts()

stat_event_user = transactions_poltava.groupby(['user_id']).agg({'event_dt' : ['count'], 'in_poltava' : ['mean', 'std'], 'dayoff' : ['mean', 'std']})
stat_event_user.columns = ['_'.join(col).strip() for col in stat_event_user.columns.values]
stat_event_user.reset_index(inplace=True)
stat_potlava = pd.merge(stat_potlava, stat_event_user, on='user_id', how='left')

stat_potlava.groupby('cluster')[['event_dt_count', 'in_poltava_mean', 'in_poltava_std', 'dayoff_mean', 'dayoff_std']].agg(['mean', 'std'])

transactions_poltava['latlon'] = np.round(transactions_poltava['lat'], 6).astype(str)+'_'+np.round(transactions_poltava['lon'], 6).astype(str)
transactions_poltava = pd.merge(transactions_poltava, stat_potlava[['user_id', 'cluster']], on='user_id', how='left')
trans_city = transactions_poltava[transactions_poltava.in_poltava==1].reset_index(drop=True)
trans_city.dropna(inplace=True)
trans_city

def top_n_point_by_cluster(trans_city, n, n_cluster, top_n, norm):

  cluster = trans_city[trans_city.cluster==n_cluster].latlon.value_counts(normalize=norm).index[:n].tolist()
  cluster_lat = [float(i.split('_')[0]) for i in cluster]
  cluster_lon = [float(i.split('_')[1]) for i in cluster]
  cluster_count = trans_city[trans_city.cluster==n_cluster].latlon.value_counts(normalize=norm).values[:n].tolist()
  unique_in_cluster = [trans_city[(trans_city.cluster==n_cluster) & (trans_city.latlon==i)].user_id.unique().tolist() for i in cluster]
  in_city_or_not = [transactions_poltava[(transactions_poltava.user_id.isin(unique_in_cluster[id]))].in_poltava.mean() for id, i in enumerate(cluster)]
  cluster_df = pd.DataFrame([cluster_lat, cluster_lon, cluster_count, [len(unique_in_cluster[id]) for id, i in enumerate(cluster)], in_city_or_not, [n_cluster]*n], index=['lat', 'lon', 'count','nunique users', 'mean geo', 'cluster']).transpose().sort_values(by=['nunique users', 'mean geo'], ascending=False).reset_index(drop=True)
  return cluster_df.loc[:top_n, :], trans_city[trans_city.cluster==n_cluster]

def point_on_map(cluster_df, df, n):
  #nominatim = Nominatim()

  m = folium.Map(location=[min(transactions_poltava[transactions_poltava.in_poltava==1]['lat']),min(transactions_poltava[transactions_poltava.in_poltava==1]['lon'])], zoom_start=12)
  colors = ['red', 'blue', 'green', 'purple', 'orange', 'darked', 'lightred', 'beige', 'darkblue', 'pink']
  for i in range(n):
    
    folium.Marker(
        location=[cluster_df.iloc[i, 0], cluster_df.iloc[i, 1]],
        tooltip=str(np.round(cluster_df.iloc[i, 2], 3))+' priority; '+str(np.round(cluster_df.iloc[i, 3], 3))+' nunique users; '+str(np.round(cluster_df.iloc[i, 4], 3))+' newcomers',
        icon=folium.Icon(color=colors[i])
    ).add_to(m)

  folium.GeoJson(
    geo_dict,
    name='Poltava geojson'
  ).add_to(m)
  folium.plugins.HeatMap(
      df[['lat','lon']].values.tolist(), 
      radius = 10,
      name='User in cluser activity'
      ).add_to(m)

  folium.LayerControl().add_to(m)
  m.save('index.html')
  return m

def map_sample_by_point(df, point, n, sample_n):
  unique_in_cluster = df[(df.latlon==point) & (df.cluster==n)].user_id.unique().tolist()
  temp = df[df.user_id.isin(sample(unique_in_cluster, sample_n))]

  trafficdf=temp.groupby(['lat','lon'])['user_id'].count()
  trafficdf=trafficdf.to_frame()
  trafficdf.columns.values[0]='count1'
  trafficdf=trafficdf.reset_index()
  lats=trafficdf[['lat','lon','count1']].values.tolist()
      
  hmap = folium.Map(location=[min(df[df.in_poltava==1]['lat']),min(df[df.in_poltava==1]['lon'])], zoom_start=10)
  hmap.add_child(HeatMap(lats, radius = 10))
  hmap.save('points.html')
  return hmap

"""### Cluster 0

1.   **Село Копылы:** красный 

  *   Развязка трассы М-03 (подъезд к Полтаве). Очень большие скопления на Полтава-Харьков, меньше на Полтава-Киев.
  * Транзитом с Кременчуга по М-22 с переездом в Копілах на М-02 до Харькова.
  *   Посадочный пункт "3 км" (поезд/електричка). Добираются с Кременчуга, Днепр и области, Миргород и тд.
  * Самые дальние соединениея: Полтава-Ужгород, Полтава-Геническ.


2.   **Автовокзал:** голубой+оранжевый

  *   Соединяется все автобусные пути, в основном Киев, Кременчуг, села в Полтавской области.
  *   Местные жители там трекаются рядом с супермаркетом Метро (нужно проверить)

3. **Село Мильці:** фиолетовый

  *   Аналогичный транзит трассы М-03 со стороны Киева, трекаем таких людей по аналичным транзакциям по пути трассы.

4. **Развязка М-03 и М-22:** зеленый

  *   Кременчуг -Харьков
  *   Половина маршрутов из Западной Украины в сторону Харькова через эту развязку
"""

info_0, trans_city0 = top_n_point_by_cluster(trans_city, 10, 0, 5, False)
point_on_map(info_0, trans_city0, 5)

info_0

map_sample_by_point(transactions_poltava, '49.58068_34.450253', 0, 100)

trafficdf=transactions_poltava[transactions_poltava.cluster==0].groupby(['lat','lon'])['user_id'].count()
trafficdf=trafficdf.to_frame()
trafficdf.columns.values[0]='count1'
trafficdf=trafficdf.reset_index()
lats=trafficdf[['lat','lon','count1']].values.tolist()
    
hmap = folium.Map(location=[min(transactions_poltava['lat']),min(transactions_poltava['lon'])], zoom_start=10)
hmap.add_child(HeatMap(lats, radius = 10))
hmap

"""### Cluster 1

1.   **Автовокзал:** красный

  *   *Соединяется все автобусные пути, в основном Киев, Кременчуг, села в Полтавской области.*
  *   *Судя по статистике транзакций в полтаве для этого кластера, то эти юзеры живут не в Полтавской области (0.08)*

2. **Посадочная станция Супрунівка:** голубая

  *   Аналогичный транзит трассы М-03 со стороны Киева, трекаем таких людей по аналичным транзакциям по пути трассы.

3. **Поселок Гужули:** зеленый

  *   хз шо
  *   Едут с Сумм, Конотопа. Не такая популярная трасса, но те, кто не транзитом едет, пользуются ей.

4. **Посадочный пункт Полтава-Киевская:** оранжевый

*   Центральная желеная дорога, едут с Миргорода, Киева И Харькова, Кременчуг+ Кривой Рог.
*   Пересадочный пункт на другие поезда.


"""

info_1, trans_city1 = top_n_point_by_cluster(trans_city, 10, 1, 5, False)
point_on_map(info_1, trans_city1, 5)

info_1

map_sample_by_point(transactions_poltava, '49.631145_34.575375', 1, 100)

trafficdf=transactions_poltava[transactions_poltava.cluster==1].groupby(['lat','lon'])['user_id'].count()
trafficdf=trafficdf.to_frame()
trafficdf.columns.values[0]='count1'
trafficdf=trafficdf.reset_index()
lats=trafficdf[['lat','lon','count1']].values.tolist()
    
hmap = folium.Map(location=[min(transactions_poltava['lat']),min(transactions_poltava['lon'])], zoom_start=10)
hmap.add_child(HeatMap(lats, radius = 10))
hmap

"""### Cluster 2
1.  **Село Копылы:** бордовый+красный
2.   **Развязка М-03 и М-22:** зеленый

  *   Соединяется все автобусные пути, в основном Киев, Кременчуг, села в Полтавской области.
  *   Судя по статистике транзакций в полтаве для этого кластера, то эти юзеры живут не в Полтавской области (0.08)

3. **Посадочная станция Супрунівка:** голубая

  *   Аналогичный транзит трассы М-03 со стороны Киева, трекаем таких людей по аналичным транзакциям по пути трассы.

4. **Поселок Ялівці:** *оранжевый*

  *   *Посадочная станция Ялівці, опять же транзит + едут из области*
  *   *Трасса Н-12, связной пункт для проезда в Суммы*


"""

info_2, trans_city2 = top_n_point_by_cluster(trans_city, 10, 2, 5, False)
point_on_map(info_2, trans_city2, 5)

info_2

map_sample_by_point(transactions_poltava, '49.631145_34.575375', 2, 100)

trafficdf=transactions_poltava[transactions_poltava.cluster==1].groupby(['lat','lon'])['user_id'].count()
trafficdf=trafficdf.to_frame()
trafficdf.columns.values[0]='count1'
trafficdf=trafficdf.reset_index()
lats=trafficdf[['lat','lon','count1']].values.tolist()
    
hmap = folium.Map(location=[min(transactions_poltava['lat']),min(transactions_poltava['lon'])], zoom_start=10)
hmap.add_child(HeatMap(lats, radius = 10))
hmap

trafficdf=transactions_poltava[transactions_poltava.cluster==2].groupby(['lat','lon'])['user_id'].count()
trafficdf=trafficdf.to_frame()
trafficdf.columns.values[0]='count1'
trafficdf=trafficdf.reset_index()
lats=trafficdf[['lat','lon','count1']].values.tolist()
    
hmap = folium.Map(location=[min(transactions_poltava['lat']),min(transactions_poltava['lon'])], zoom_start=10)
hmap.add_child(HeatMap(lats, radius = 10))
hmap

# pip install OSMPythonTools

# from geopy.geocoders import Nominatim

# geolocator = Nominatim()

# df = transactions_poltava.sample(100)
# df['address'] = df.apply(
#     lambda row: nominatim.query(row['lat'], row['lon'], reverse=True, zoom=16).address()['state'], axis=1)

"""## Task 2.3: Purpose of the trip"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from geopy.distance import great_circle
from shapely.geometry import MultiPoint
from datetime import datetime as dt

pip install hdbscan

import hdbscan

unique_location = trans_city.latlon.value_counts()
unique_location = unique_location[unique_location>100].index.tolist()

trans_clust = trans_city[trans_city['latlon'].isin(unique_location)].drop_duplicates(subset=['latlon'])

trans_clust.latlon.nunique()

X_trans = StandardScaler().fit_transform(trans_clust[['lat', 'lon']])
temp = trans_clust[['lat', 'lon', 'latlon']]

import hdbscan
clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5)
clusterer.fit(temp[['lat', 'lon']])

temp['cluster_location'] = clusterer.labels_
clusterer.labels_.max()

fig = plt.figure(figsize=(6,6))
sns.scatterplot(x='lat', y='lon', hue='cluster_location',data=temp)
plt.title('Clusters of locations')
plt.savefig('hdbscan.png')
plt.show()

transactions_poltava_clust = transactions_poltava.merge(temp[['latlon', 'cluster_location']], on='latlon', how='left')
user_trans_cluster = transactions_poltava_clust.groupby('user_id')['cluster_location'].value_counts().to_frame().rename(columns={'cluster_location':'count_cluster'}).reset_index().drop_duplicates(['user_id'], keep='first')[['user_id', 'cluster_location']]
user_trans_cluster['user_cluster'] = user_trans_cluster['cluster_location']
transactions_poltava_clust11 = transactions_poltava_clust.merge(user_trans_cluster[['user_id', 'user_cluster']], on='user_id', how='left')

user_trans_cluster.cluster_location.value_counts()

"""Кластер 0 на выходных приездает с целями покупок (магазины Метро, Вещевой рынок). часто приезжают на автобусе. (или это продавцы из Западной Украины)."""

temp = transactions_poltava_clust11[(transactions_poltava_clust11.dayoff==1)]

trafficdf=temp.groupby(['lat','lon'])['user_id'].count()
trafficdf=trafficdf.to_frame()
trafficdf.columns.values[0]='count1'
trafficdf=trafficdf.reset_index()
lats=trafficdf[['lat','lon','count1']].values.tolist()
    
hmap = folium.Map(location=[min(temp[temp.in_poltava==1]['lat']),min(temp[temp.in_poltava==1]['lon'])], zoom_start=10)
hmap.add_child(HeatMap(lats, radius = 10))
hmap

"""## Graph

### IGraph

### NetworkX
"""

all_move = routes_19[['start_area_id', 'finish_area_id']].values.tolist()
str_all = []
for i in range(len(all_move)):
  all_move[i].sort()
  str_all.append(str(all_move[i][0])+'_'+str(all_move[i][1]))

create_weight = pd.DataFrame({'nodes': str_all, 'date': routes_19['start_time'].values})

value_count = create_weight['nodes'].value_counts()
all_set = len(create_weight)

all_point = routes_19['start_area_id'].values.tolist() + routes_19['finish_area_id'].values.tolist()

from collections import Counter
all_point = Counter(all_point)

G = nx.Graph(directed=False)
for move in create_weight['nodes'].unique():
  w = value_count[move] / all_set
  if w>=5*10**(-6):
    G.add_edge(int(move.split('_')[0]), int(move.split('_')[1]), 
             weight=w,
             )

# c_betweenness = nx.betweenness_centrality(G)
# c_betweenness = list(c_betweenness.values())

# c_degree = nx.degree_centrality(G)
# c_degree = list(c_degree.values())

# c_eigenvector = nx.eigenvector_centrality(G)
# c_eigenvector = list(c_eigenvector.values())

# c_closeness = nx.closeness_centrality(G)
# c_closeness = list(c_closeness.values())

# pos = nx.spring_layout(G)

# plt.figure(figsize=(18, 12))# Degree Centrality
# f, axarr = plt.subplots(2, 2, num=1)
# plt.sca(axarr[0,0])
# nx.draw(G, cmap = plt.get_cmap('inferno'), node_color = c_degree, node_size=300, pos=pos, with_labels=True)
# axarr[0,0].set_title('Degree Centrality', size=16)# Eigenvalue Centrality
# plt.sca(axarr[0,1])
# nx.draw(G, cmap = plt.get_cmap('inferno'), node_color = c_eigenvector, node_size=300, pos=pos, with_labels=True)
# axarr[0,1].set_title('Eigenvalue Centrality', size=16)# Proximity Centrality
# plt.sca(axarr[1,0])
# nx.draw(G, cmap = plt.get_cmap('inferno'), node_color = c_closeness, node_size=300, pos=pos, with_labels=True)
# axarr[1,0].set_title('Proximity Centrality', size=16)# Betweenness Centrality
# plt.sca(axarr[1,1])
# nx.draw(G, cmap = plt.get_cmap('inferno'), node_color = c_betweenness, node_size=300, pos=pos, with_labels=True)
# axarr[1,1].set_title('Betweenness Centrality', size=16)

# import random

# edge_subset = random.sample(G.edges(), int(0.25 * G.number_of_edges()))# Remove some edges
# G_karate_train = G.copy()
# G_karate_train.remove_edges_from(edge_subset)

def simple_Louvain(G):
    """ Louvain method github basic example"""
    partition = community.best_partition(G)
    pos = graphviz_layout(G)
    
    max_k_w = []
    for com in set(partition.values()):
        list_nodes = [nodes for nodes in partition.keys()
                      if partition[nodes] == com]
        max_k_w = max_k_w + [list_nodes]

    
    node_mapping = {}
    map_v = 0
    for node in G.nodes():
        node_mapping[node] = map_v
        map_v += 1

    community_num_group = len(max_k_w)
    color_list_community = [[] for i in range(len(G.nodes()))]
    
    # color
    for i in G.nodes():
        for j in range(community_num_group):
            if i in max_k_w[j]:
                color_list_community[node_mapping[i]] = j
    
    return G, pos, color_list_community, community_num_group, max_k_w

import pydot
from networkx.drawing.nx_pydot import graphviz_layout

G, pos, color_list_community, community_num_group, max_k_w = simple_Louvain(G)

edges = G.edges()
Feature_color_sub = color_list_community
node_size = 150
#node_size = [value / 100 for value in all_point.values()]
fig = plt.figure(figsize=(10, 8))
im = nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=Feature_color_sub, cmap='jet', vmin=0, vmax=community_num_group)
nx.draw_networkx_edges(G, pos, alpha=0.25, width=0.15)
nx.draw_networkx_labels(G, pos, font_size=12, font_color="black")
plt.xticks([])
plt.yticks([])
plt.colorbar(im)
plt.title('Louvain method')
plt.savefig('graph.png')
plt.show()

color_list_community[:5]

trafficdf=districts[districts.area_id.isin(max_k_w[0])][['centroid_lat','centroid_lon']]
#trafficdf=trafficdf.to_frame()
#trafficdf.columns.values[0]='count1'
trafficdf=trafficdf.reset_index()
lats=trafficdf[['centroid_lat','centroid_lon']].values.tolist()
    
hmap = folium.Map(location=[min(trafficdf['centroid_lat']),min(trafficdf['centroid_lon'])], zoom_start=6)
hmap.add_child(HeatMap(lats, radius = 10))
hmap

"""## Insight"""

# routes_19, routes_20

routes_19.head(3)

"""### 1"""

ts_19 = routes_19.groupby(['date'])['start_time'].count().values
ts_20 = routes_20.groupby(['date'])['start_time'].count().values
plt.plot(ts_19, label='2019')
plt.plot(ts_20, label='2020')
plt.title('Day count of routes 2019/2020')
plt.legend()
plt.show()

"""### 2"""

ts_19 = routes_19.groupby(['date'])['duration_move'].mean().values
ts_20 = routes_20.groupby(['date'])['duration_move'].mean().values
plt.plot(ts_19, label='2019')
plt.plot(ts_20, label='2020')
plt.title('Day mean durationn move of routes 2019/2020')
plt.legend()
plt.show()

"""### 3"""

ts_19 = routes_19[routes_19.same_region==0].groupby(['date'])['duration_move'].mean().values
ts_20 = routes_20[routes_20.same_region==0].groupby(['date'])['duration_move'].mean().values
plt.plot(ts_19, label='2019')
plt.plot(ts_20, label='2020')
plt.title('Day mean durationn move on different regions of routes 2019/2020')
plt.legend()
plt.show()

"""### 4"""

ts_19 = routes_19[routes_19.same_region==0].groupby(['date'])['duration_move'].count().values
ts_20 = routes_20[routes_20.same_region==0].groupby(['date'])['duration_move'].count().values
plt.plot(ts_19, label='2019')
plt.plot(ts_20, label='2020')
plt.title('Day count move on different regions of routes 2019/2020')
plt.legend()
plt.show()

ts_19 = routes_19[routes_19.same_region==1].groupby(['date'])['duration_move'].count().values
ts_20 = routes_20[routes_20.same_region==1].groupby(['date'])['duration_move'].count().values
plt.plot(ts_19, label='2019')
plt.plot(ts_20, label='2020')
plt.title('Day count move on different regions of routes 2019/2020')
plt.legend()
plt.show()

"""### 5"""

ts_19 = routes_19.groupby(['region_name', 'date'])['duration_move'].count()
ts_20 = routes_20.groupby(['region_name', 'date'])['duration_move'].count()
for area in routes_19.region_name.unique():
  plt.plot(ts_19[area].values, label='2019')
  plt.plot(ts_20[area].values, label='2020')
  plt.title(f'Day count of routes 2019/2020 in {area} oblast')
  plt.legend()
  plt.show()

ts_19 = routes_19[routes_19.month==5].groupby(['date'])['duration_move'].sum().values
ts_20 = routes_20[routes_20.month==5].groupby(['date'])['duration_move'].sum().values
plt.plot(ts_19, label='2019')
plt.plot(ts_20, label='2020')
plt.title('Day count move on 1 Maya 2019/2020')
plt.legend()
plt.show()