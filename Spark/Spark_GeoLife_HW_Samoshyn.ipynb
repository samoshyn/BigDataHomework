{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Azure Blob Storage Import Files",
      "dashboards": [],
      "language": "python",
      "widgets": {},
      "notebookOrigID": 3008351295669162
    },
    "colab": {
      "name": "Spark_GeoLife_HW_Samoshyn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_57Q_QEDfkC"
      },
      "source": [
        "# Домашнее задание BigData School Самошина Андрея\n",
        "**Дано:** логи пользователей с GPS-данными.\n",
        "\n",
        "Необходимо провести чистку данных, выбрать только тех, у кого размеченые логи и предсказать часть логов. Это задача мульти-класс классификация, потому что одному логу можно сопоставить только однку метку какого-либо класса.\n",
        "\n",
        "Работа выполнена частично локально (чистка и структурирование данных), остальная работа уже с использованием blob storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7PCe2WcEw7q"
      },
      "source": [
        "## Работа с входными данными\n",
        "\n",
        "После проведения анализа данных было замечено пару особенностей:\n",
        "\n",
        "*   Не все пользователи имеют разметку таргета (мы их не будем учитывать)\n",
        "*   Даже у тех, у кого есть разметка, не всегда совпадает с количеством логов (тоже нужно отфильтровать)\n",
        "\n",
        "Скрипты были написаны локально, потому в этом ноутбуке презентую только их вид и описание. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbBnFpk1Fn1w"
      },
      "source": [
        "### Convert Labels to .csv\n",
        "\n",
        "Размеченные таргеты в исходном формате предоставлены в формате .txt, что достаточно неудобно. Сконвертируем их в csv. Проходим по всем файлам в папках и если встречается файл с названием labels, то конвертируем его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siVxCr2pF3Pf"
      },
      "source": [
        "import csv, os, sys\n",
        "\n",
        "def convertToCSV(root, direc):\n",
        "\n",
        "    with open(root + '/' + direc + '/' + 'labels.txt','rt') as fin:\n",
        "        cr = csv.reader(fin, delimiter='\\t')\n",
        "        filecontents = [line for line in cr]\n",
        "    with open(root + '/' + direc + '/' + 'labels_csv.csv','w') as fou:\n",
        "    \tcw = csv.writer(fou, lineterminator='\\n')\n",
        "    \tcw.writerows(filecontents)\n",
        "\n",
        "\n",
        "thedir = \"D:\\\\Big_Data_School_5\\DataFrame\\Lesson_2\\Geolife_Trajectories_1_3\\Data_copy\"\n",
        "\n",
        "dirs = [name for name in os.listdir(thedir) if os.path.isdir(os.path.join(thedir, name))]\n",
        "\n",
        "for direc in dirs:\n",
        "\tif 'labels.txt' in os.listdir(thedir + '/' + direc):\n",
        "\t\tconvertToCSV(thedir, direc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QY0grC7GFiP"
      },
      "source": [
        "### Preprocessing logs\n",
        "\n",
        "Логи в исходном формате тоже в неудобном формате, к тому же первые 6 строк содержат чисто техническую информацию. Обработаем это и также добавим названия колонок для дальнейней работы с ними."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0EEG1gdGnEl"
      },
      "source": [
        "import csv, os, sys\n",
        "\n",
        "colNames = ['Latitude','Longitude','AllZero','Altitude','NumberOfDays','Date','Time']\n",
        "\n",
        "def cleanPlt(root, direc1, direc2, colNames):\n",
        "\n",
        "\t# read plt file\n",
        "\twith open(root + '/' + direc1 + '/Trajectory/' + direc2,'rt') as fin:\n",
        "\t\tcr = csv.reader(fin)\n",
        "\t\tfilecontents = [line for line in cr][6:]\n",
        "\t\tfilecontents.insert(0, colNames)\n",
        "\n",
        "\t# write csv file without header\n",
        "\twith open(root +'/' + direc1 + '/Trajectory/' + direc2[:-4] + '.csv','w') as fou:\n",
        "\t\tcw = csv.writer(fou, lineterminator='\\n')\n",
        "\t\tcw.writerows(filecontents)\n",
        "\n",
        "\tos.remove(root + '/' + direc1 + '/Trajectory/' + direc2)\n",
        "\n",
        "thedir = \"D:\\\\Big_Data_School_5\\DataFrame\\Lesson_2\\Geolife_Trajectories_1_3\\Data_copy\"\n",
        "\n",
        "dirs = [name for name in os.listdir(thedir) if os.path.isdir(os.path.join(thedir, name))]\n",
        "\n",
        "for direc in dirs:\n",
        "    if 'labels_csv.csv' in os.listdir(thedir + '/' + direc):\n",
        "       \tprint('Cleaning:', direc)\n",
        "       \ttempdirs = os.listdir(thedir + '/' + direc + '/Trajectory')\n",
        "       \tsubdirs = []\n",
        "       \tfor item in tempdirs:\n",
        "       \t\tif not item.endswith('.DS_Store'):\n",
        "       \t\t\tsubdirs.append(item)\n",
        "       \tfor subdir in subdirs:\n",
        "       \t\tcleanPlt(thedir, direc, subdir, colNames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21nDvUaKGt6K"
      },
      "source": [
        "### Detect start times in labels\n",
        "\n",
        "Идея заключается в том, что б обрезать логи только до того времени, которое есть в размеченом датасете, а остальные данные пропустить. Я посчитал, что таким образом потеряется порядка 7% данных с разметкой, что сравнительно немного. В результате секономим довольно много времени, а на количестве данных для обучения это не сильно скажется."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQby5nkHcAs"
      },
      "source": [
        "import csv, os, sys\n",
        "\n",
        "def cleanLabels(root, direc):\n",
        "\n",
        "\twith open(root + '/' + direc + '/labels_csv.csv','rt') as fin:\n",
        "\t\tcr = csv.reader(fin)\n",
        "\t\tlabelContents = [line for line in cr]\n",
        "        #print(labelContents)\n",
        "\t\tstartTimes = [entry[0] for entry in labelContents[1:]]\n",
        "\t\tstartLabels = [time.replace('/','').replace(' ','').replace(':','') + '.csv' for time in startTimes]\n",
        "\n",
        "\ttrajs = os.listdir(root + '/' + direc + '/Trajectory')\n",
        "\n",
        "\tfor traj in trajs:\n",
        "\t\tif traj not in startLabels:\n",
        "            \n",
        "\t\t\tos.remove(root + '/' + direc + '/Trajectory/' + traj)\n",
        "\n",
        "\tfilecontents = [labelContents[0]] #the new labels.csv to be written\n",
        "\n",
        "\tfor i in range(len(startLabels)):\n",
        "\t\tif startLabels[i] in trajs:\n",
        "\t\t\tfilecontents.append(labelContents[i+1])\t\n",
        "\n",
        "\tos.remove(root + '/' + direc + '/labels_csv.csv')\n",
        "\n",
        "\twith open(root + '/' + direc + '/labels_csv.csv','w') as fou:\n",
        "\t\tcw = csv.writer(fou, lineterminator='\\n')\n",
        "\t\tcw.writerows(filecontents)\n",
        "\n",
        "thedir = \"D:\\\\Big_Data_School_5\\DataFrame\\Lesson_2\\Geolife_Trajectories_1_3\\Data_copy\"\n",
        "\n",
        "dirs = [name for name in os.listdir(thedir) if os.path.isdir(os.path.join(thedir, name))]\n",
        "\n",
        "for direc in dirs:\n",
        "    if 'labels_csv.csv' in os.listdir(thedir + '/' + direc):\n",
        "        cleanLabels(thedir, direc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGpC2PqHktd"
      },
      "source": [
        "### Sorted data\n",
        "\n",
        "Осталось только отсортировать логи, которые имеют разметку и сопоставляются по времени фиксации данных в разметке, остальные логи удалить. А также перенести в логи метку таргета по соотвествующему промежутку времени в файле labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Uqkh9fICim"
      },
      "source": [
        "import csv, os, sys\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "\n",
        "def labelTraj(root, direc):\n",
        "\n",
        "\tldf = pd.read_csv(root + '/' + direc + '/labels_csv.csv')\n",
        "\tldf['Start Time'], ldf['End Time']  = pd.to_datetime(ldf['Start Time']), pd.to_datetime(ldf['End Time'])\n",
        "\t\n",
        "\ttrajs = os.listdir(root + '/' + direc + '/Trajectory')\n",
        "\ttrajs = [traj for traj in trajs if traj != '.DS_Store']\n",
        "\n",
        "\tos.makedirs(root + '/' + direc + '/Trajectory/Labelled/')\n",
        "\tos.makedirs(root + '/' + direc + '/Trajectory/Unlabelled/')\n",
        "\n",
        "\tlabelled = []\n",
        "\tunlabelled= []\n",
        "\n",
        "\ti = 0\n",
        "\n",
        "\tfor index, row in ldf.iterrows():\n",
        "\n",
        "\t\tif i == len(trajs):\n",
        "\t\t\ti = 0\n",
        "\n",
        "\t\twhile i < len(trajs):\n",
        "\n",
        "\t\t\ttraj = trajs[i]\n",
        "\n",
        "\t\t\ttdf = pd.read_csv(root + '/' + direc + '/Trajectory/' + traj)\n",
        "\t\t\ttdf['datetime'] = tdf.Date + ' ' + tdf.Time\n",
        "\t\t\ttdf['datetime'] = pd.to_datetime(tdf.datetime)\n",
        "\n",
        "\t\t\tpremask = (tdf['datetime'] <= ldf['Start Time'].iloc[index])\n",
        "\t\t\tpreSubTraj = tdf[premask].copy()\n",
        "\n",
        "\t\t\tmask = (tdf['datetime'] <= ldf['End Time'].iloc[index]) & (tdf['datetime'] >= ldf['Start Time'].iloc[index])\n",
        "\t\t\tsubTraj = tdf[mask].copy()\n",
        "\n",
        "\t\t\tif len(subTraj) != 0:\n",
        "\t\t\t\tsubTraj['Transportation Mode'] = str(ldf['Transportation Mode'].iloc[index])\n",
        "\t\t\t\tname = str(subTraj['datetime'].iloc[0]).replace('-','').replace(' ','').replace(':','') + '.csv'\n",
        "\t\t\t\tsubTraj.to_csv(root + '/' + direc + '/Trajectory/Labelled/' + name, index=False)\n",
        "\t\t\t\tlabelled.append(name)\n",
        "\n",
        "\t\t\t\tif len(preSubTraj) != 0:\n",
        "\t\t\t\t\tname = str(preSubTraj['datetime'].iloc[0]).replace('-','').replace(' ','').replace(':','') + '.csv'\n",
        "\t\t\t\t\tif name not in labelled and name not in unlabelled:\n",
        "\t\t\t\t\t\tpreSubTraj['Transportation Mode'] = '-'\n",
        "\t\t\t\t\t\tpreSubTraj.to_csv(root + '/' + direc + '/Trajectory/Unlabelled/' + name, index=False)\n",
        "\t\t\t\t\t\tunlabelled.append(name)\n",
        "\n",
        "\t\t\t\tif index + 1 == len(ldf):\n",
        "\t\t\t\t\tstartIndex = subTraj.index[-1] + 1\n",
        "\t\t\t\t\tif startIndex > tdf.index[-1]:\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tnolabelSubTraj = tdf[startIndex:].copy()\n",
        "\t\t\t\t\tnolabelSubTraj['Transportation Mode'] = '-'\n",
        "\t\t\t\t\tname = str(tdf['datetime'].iloc[startIndex]).replace('-','').replace(' ','').replace(':','') + '.csv'\n",
        "\t\t\t\t\tnolabelSubTraj.to_csv(root + '/' + direc + '/Trajectory/Unlabelled/' + name, index=False)\n",
        "\t\t\t\t\tunlabelled.append(name)\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tif ldf['Start Time'].iloc[index+1] <= tdf['datetime'].iloc[-1]:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tstartIndex = subTraj.index[-1] + 1\n",
        "\t\t\t\t\tif startIndex > tdf.index[-1]:\n",
        "\t\t\t\t\t\ti += 1\n",
        "\t\t\t\t\t\tcontinue \n",
        "\t\t\t\t\tnolabelSubTraj = tdf[startIndex:].copy()\n",
        "\t\t\t\t\tnolabelSubTraj['Transportation Mode'] = '-'\n",
        "\t\t\t\t\tname = str(tdf['datetime'].iloc[startIndex]).replace('-','').replace(' ','').replace(':','') + '.csv'\n",
        "\t\t\t\t\tnolabelSubTraj.to_csv(root + '/' + direc + '/Trajectory/Unlabelled/' + name, index=False)\n",
        "\t\t\t\t\tunlabelled.append(name)\n",
        "\t\t\t\t\ti += 1\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tif index + 1 == len(ldf):\n",
        "\t\t\t\tbreak\t\t\t\n",
        "\n",
        "\t\t\telif ldf['Start Time'].iloc[index + 1] > tdf['datetime'].iloc[-1] and traj not in unlabelled:\n",
        "\t\t\t\ttdf['Transportation Mode'] = '-'\n",
        "\t\t\t\ttdf.to_csv(root + '/' + direc + '/Trajectory/Unlabelled/' + traj, index=False)\n",
        "\t\t\t\tunlabelled.append(traj)\n",
        "\t\t\t\ti += 1\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tif len(subTraj) != 0:\n",
        "\t\t\t\tif subTraj.index[-1] == tdf.index[-1]:\n",
        "\t\t\t\t\ti += 1\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tcontinue\n",
        "\n",
        "def removeOriginals(root, direc):\n",
        "\n",
        "\tfiles = [name for name in os.listdir(thedir + '/' + direc + '/Trajectory') if os.path.isfile(os.path.join(thedir + '/' + direc + '/Trajectory', name))]\n",
        "\tfiles = [file for file in files if file != '.DS_Store']\n",
        "\n",
        "\tfor file in files:\n",
        "\t\tos.remove(root + '/' + direc + '/Trajectory/' + file)\n",
        "\n",
        "thedir = \"D:\\\\Big_Data_School_5\\DataFrame\\Lesson_2\\Geolife_Trajectories_1_3\\Data_copy\"\n",
        "\n",
        "dirs = [name for name in os.listdir(thedir) if os.path.isdir(os.path.join(thedir, name))]\n",
        "\n",
        "for direc in dirs:\n",
        "\t\n",
        "    if 'labels_csv.csv' in os.listdir(thedir + '/' + direc):\n",
        "        print('Labels User:', direc) \n",
        "        shutil.move(thedir+ '/' + direc + '/Trajectory/Labelled', thedir+ '/' + direc + '/Traj_Labelled')\n",
        "        shutil.rmtree(thedir+ '/' + direc + '/Trajectory')\n",
        "    else:\n",
        "        print('Unlabels User:', direc)\n",
        "        shutil.rmtree(thedir+ '/' + direc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v3IdjyEISzg"
      },
      "source": [
        "### Upload to Blob\n",
        "\n",
        "Осталось все полученное загрузить в блоб. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VXnhe1RImo5"
      },
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient,PublicAccess\n",
        "import os\n",
        "\n",
        "\n",
        "def upload_to_blob(r, path_remove, file, container):\n",
        "    file_path_on_azure = os.path.join(r,file).replace(path_remove,\"\")\n",
        "    file_path_on_local = os.path.join(r,file)\n",
        "    #print(file_path_on_local)\n",
        "\n",
        "    blob_client = container.get_blob_client(file_path_on_azure)\n",
        "\n",
        "    with open(file_path_on_local,'rb') as data:\n",
        "        blob_client.upload_blob(data)\n",
        "\n",
        "def run_sample():    \n",
        "    conn_str=\"DefaultEndpointsProtocol=https;AccountName=mlstorageall;AccountKey=djvx9juh6EJZWqx7T3U2D61BwhPWyEx4BcVyzbt/AQhaNR8hGmVBtzaJnbbG2w4c67p4dZkZIAWK6kq1PbV8Zg==;EndpointSuffix=core.windows.net\"\n",
        "    container_name=\"samoshyn\"    \n",
        "    \n",
        "    path_remove = \"D:\\\\Big_Data_School_5\\DataFrame\\Lesson_2\\Geolife_Trajectories_1_3\\\\\"\n",
        "    local_path = \"D:\\\\Big_Data_School_5\\DataFrame\\Lesson_2\\Geolife_Trajectories_1_3\\AllData_preprocess\" #the local folder\n",
        "\n",
        "    service_client=BlobServiceClient.from_connection_string(conn_str)\n",
        "    container_client = service_client.get_container_client(container_name)  \n",
        "    \n",
        "    file_check = 0\n",
        "    file = None\n",
        "    total = 0\n",
        "    for r,d,f in os.walk(local_path):\n",
        "        \n",
        "        if file_check==1:\n",
        "            #print(r)\n",
        "            total = total + 1\n",
        "            for file in f:\n",
        "                #print(os.path.join(r,file))\n",
        "                upload_to_blob(r, path_remove, file, container_client)\n",
        "            print('Upload user:', total)\n",
        "            file_check = 0\n",
        "        if f:\n",
        "            \n",
        "            for file in f:\n",
        "               #print(file)    \n",
        "               if file=='labels_csv.csv':\n",
        "                    #print(os.path.join(r,file))\n",
        "                    upload_to_blob(r, path_remove, file, container_client)\n",
        "                    file_check = 1\n",
        "    print('----Total users:', total)\n",
        "\n",
        "\n",
        "run_sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6lxB44MJZsV"
      },
      "source": [
        "На выходе получаем все данные, загруженные в блоб сторедж с сохранением иерархичности папок.\n",
        "\n",
        "![1](https://drive.google.com/uc?export=view&id=1Q0LYBPWx5c9ScuVpilehj5ErtcbPMXr2)\n",
        "\n",
        "**Заметка:** *К сожалению, у меня не получилось прочитать в Google Colab файлы из стореджа (при аналогичной операции в Датабриксе проблем нету), хотя пройтись по ним я могу без проблем, об этом ниже. И поскольку я работал локально, то использовал загруженный архив этих же почищенных данных.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jFtt5U1BcLf"
      },
      "source": [
        "## Установка Spark и инициализация\n",
        "\n",
        "По техническим причинам Датабрикс недоступен длительное время, потому я решил работать в Google Colab (community version Датабрикс был довольно слабый). При таком раскладе я не могу работать над дополнительным заданием в виде считывания данных потоками, но лучше сделать костяк и в случае чего доделать в другой среде. Дополнительно нужно было установить Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbplVzOaGWSn"
      },
      "source": [
        "# Run below commands in google colab\n",
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark3.0.0\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# unzip it\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# install findspark \n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhSqK19hG8K2"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jbNIOP2s_be"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCt5sz9ftnmO"
      },
      "source": [
        "!pip install azure-storage-blob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUAMgkUFBhsH"
      },
      "source": [
        "## Подключаем сторедж"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrlqPCoB-39"
      },
      "source": [
        "!pip install azure-storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "578a4b05-5381-45ce-83d9-a16586580d2d"
        },
        "id": "ooLhacBBrS4o"
      },
      "source": [
        "storage_account_name = 'mlstorageall'\n",
        "storage_account_access_key = 'djvx9juh6EJZWqx7T3U2D61BwhPWyEx4BcVyzbt/AQhaNR8hGmVBtzaJnbbG2w4c67p4dZkZIAWK6kq1PbV8Zg=='\n",
        "spark.conf.set('fs.azure.account.key.' + storage_account_name + '.blob.core.windows.net', storage_account_access_key)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e13ab1b1-9022-4524-bbae-756bd70a070a"
        },
        "id": "RGVrbkD4rS4p"
      },
      "source": [
        "blob_container = 'samoshyn'\n",
        "#general_folder = 'TestFolder'\n",
        "general_folder = 'AllData_preprocess'\n",
        "filePath = \"wasbs://\" + blob_container + \"@\" + storage_account_name + \".blob.core.windows.net/\" + general_folder"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMNeFSXdKnIJ"
      },
      "source": [
        "Проверяем, что мы подключились куда нужно и выведем пути всех нужных логов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "05903741-9cda-41a4-9331-bd37c30a057a"
        },
        "id": "_ju95kRprS4p"
      },
      "source": [
        "import azure.storage\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=mlstorageall;AccountKey=djvx9juh6EJZWqx7T3U2D61BwhPWyEx4BcVyzbt/AQhaNR8hGmVBtzaJnbbG2w4c67p4dZkZIAWK6kq1PbV8Zg==;EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "container_client=blob_service_client.get_container_client(blob_container)\n",
        "\n",
        "blob_list = container_client.list_blobs(name_starts_with=\"AllData_preprocess/\")\n",
        "for blob in blob_list:\n",
        "  path_list = blob.name.split(os.sep)\n",
        "  #print(path_list)\n",
        "  #print(blob.name)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccy1K7jNKwOG"
      },
      "source": [
        "Дальше работаем с данными с архива о котором я упомянул выше."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equliIHk1Jcg"
      },
      "source": [
        "#!unzip -q /content/TestFolder.zip\n",
        "!unzip -q /content/AllData_preprocess.zip"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGkYTO4PK5jU"
      },
      "source": [
        "## Предобработка данных\n",
        "\n",
        "Ниже представлены все функции, которые я использовал для работы с данными и их применение на одном логе для эффективной траты времени. Дальше будем масштабировать на всех юзеров.\n",
        "\n",
        "Для начала правильно считаем данные и выведем полученный датафрейм. Я сразу удалил несколько столбцов ('Date', 'Time', 'AllZero', 'NumberOfDays') и заменил их одной фичей datetime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d6bb0b45-d7d9-4c84-a56a-d350d10b3199"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix8WDkh9rS4p",
        "outputId": "f728c3e8-6e66-4c3d-dd63-0597a79c436b"
      },
      "source": [
        "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, FloatType, DataType, DateType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "import numpy as np\n",
        "import math\n",
        "import pyspark.sql.types as t\n",
        "from itertools import chain\n",
        "\n",
        "#filePath = 'TestFolder'\n",
        "filePath = 'AllData_preprocess'\n",
        "def_path = '/010/Traj_Labelled/20080403160000.csv'\n",
        "\n",
        "geoSchema = StructType([ \\\n",
        "      StructField(\"Latitude\", FloatType(), True), \\\n",
        "      StructField(\"Longitude\", FloatType(), True), \\\n",
        "      StructField(\"AllZero\", IntegerType(), True), \\\n",
        "      StructField(\"Altitude\", IntegerType(), True), \\\n",
        "      StructField(\"NumberOfDays\", FloatType(), True), \\\n",
        "      StructField(\"Date\", DateType(), True), \\\n",
        "      StructField(\"Time\", StringType(), True), \\\n",
        "      StructField(\"datetime\", StringType(), True), \\\n",
        "      StructField(\"Transportation Mode\", StringType(), True)])\n",
        "\n",
        "def readcsv(path, schema):\n",
        "\n",
        "  infer_schema = \"false\"\n",
        "  first_row_is_header = \"true\"\n",
        "  delimiter = \",\"\n",
        "  file_type = \"csv\"\n",
        "\n",
        "  df = spark.read.format(file_type) \\\n",
        "    .option(\"inferSchema\", infer_schema) \\\n",
        "    .option(\"header\", first_row_is_header) \\\n",
        "    .option(\"sep\", delimiter) \\\n",
        "    .schema(geoSchema) \\\n",
        "    .load(filePath+path)\n",
        "\n",
        "  drop_list = ['Date', 'Time', 'AllZero', 'NumberOfDays']\n",
        "  df = df.select([column for column in df.columns if column not in drop_list])\n",
        "  \n",
        "  return df\n",
        "\n",
        "df_expample = readcsv(def_path, geoSchema)\n",
        "df_expample.show(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------+-------------------+-------------------+\n",
            "| Latitude|Longitude|Altitude|           datetime|Transportation Mode|\n",
            "+---------+---------+--------+-------------------+-------------------+\n",
            "|41.765053| 83.34479|    -777|2008-04-03 16:00:00|              train|\n",
            "|41.765114|83.345116|    -777|2008-04-03 16:00:01|              train|\n",
            "|41.765175| 83.34545|    -777|2008-04-03 16:00:02|              train|\n",
            "| 41.76523|83.345795|    -777|2008-04-03 16:00:03|              train|\n",
            "| 41.76528| 83.34614|    -777|2008-04-03 16:00:04|              train|\n",
            "+---------+---------+--------+-------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fvQt2lbLuqT"
      },
      "source": [
        "Вспомогательные функции для генерации фич: переход в Декартову систему координат через сферические, поиск расстояния между двумя точками и поиск угла смещения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a290876e-bd91-49bc-ab8d-85ff35321676"
        },
        "id": "oa0VdkUNrS4p"
      },
      "source": [
        "def get_cartesian(lat,lon):\n",
        "    lat = lat*math.pi/180\n",
        "    lon = lon*math.pi/180\n",
        "    R = 6371000\n",
        "    x = R * math.cos(lat) * math.cos(lon)\n",
        "    y = R * math.cos(lat) * math.sin(lon)\n",
        "    return t.Row('Out1', 'Out2')(x,y)\n",
        "  \n",
        "def dist(long_x, lat_x, long_y, lat_y):\n",
        "  if long_y==None or lat_y==None:\n",
        "    return float(0.01) \n",
        "  dx = math.pow((long_x-long_y),2)\n",
        "  dy =  math.pow((lat_x-lat_y),2)\n",
        "  return  math.pow((dx+dy), 0.5)\n",
        "\n",
        "def angle(x0, x1, y0, y1):\n",
        "  if (x0==0 and x1==0) or (y0==0 and y1==0):\n",
        "    return float(0.01) \n",
        "  lenx =  math.pow(( math.pow(x0,2) + math.pow(x1,2)), 0.5)\n",
        "  leny =  math.pow(( math.pow(y0,2) + math.pow(y1,2)), 0.5)\n",
        "  \n",
        "  return math.acos((x0 * y0 + x1 * y1)/ (lenx * leny))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PUL-1CSMEbt"
      },
      "source": [
        "**Теперь приступаем к генерации фич:** \n",
        "\n",
        "*   Для начала, переведем координаты в декартовы, что б можно было легко считать расстояния **(X_coord, Y_coord)**.\n",
        "*   Дальше считаем длины между двумя соседними кординатами (в пределах одного лога) путем взятия так называемого лага (сдвига) нашей координаты и дальнейшей передачи этих точек в функцию для поиска длины. Получим расстояние, которое преодолел юзер в промежуток времени [Tn, Tn+1] **(Dist)**.\n",
        "*   Аналогично считаем продолжительнось промежутка времени [Tn, Tn+1].\n",
        "*   И ищем **скорость** путем деления Dist на полученное время.\n",
        "*   (Протестировано) Существенного приоста следующая фича не дала, но для отчетности добавлю: ищем **угол смещения** с точки А в точку Б. Т.е. насколько у нас изменилось направление траектории. По-идее, будет влиять на определение самолетов и метро(они в основном не сильно петляют), но не помогло. Потому для экономии времени я её не использовал.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "19929c32-2f21-4f8f-a0c4-4f8c443805f2"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt8UsPTzrS4q",
        "outputId": "e1fc59be-67bf-4e06-82e8-dccec70c2df4"
      },
      "source": [
        "def FeatureEng(df):\n",
        "  \n",
        "  schema = StructType([\n",
        "    StructField(\"X_coord\", FloatType(), False),\n",
        "    StructField(\"Y_coord\", FloatType(), False)])\n",
        "    \n",
        "  get_cartesian_udf = udf(lambda x,y: get_cartesian(x,y), schema)\n",
        "  df = df.withColumn(\"Output\", get_cartesian_udf('Latitude', 'Longitude'))\n",
        "  df = df.select(\"Latitude\", \"Longitude\", \"Altitude\", \"datetime\", \"Transportation Mode\", \"Output.*\")\n",
        "\n",
        "  get_dist_udf = udf(lambda x,y,z,g: dist(x,y,z,g), FloatType())\n",
        "  df = df.withColumn(\"Dist\", get_dist_udf(\n",
        "      \"X_coord\", \"Y_coord\",\n",
        "      lag(\"X_coord\", 1).over(Window().orderBy(\"datetime\")), lag(\"Y_coord\", 1).over(Window().orderBy(\"datetime\"))))\n",
        "  \n",
        "  df = df.withColumn(\"lag_time\", lag(\"datetime\", 1).over(Window().orderBy(\"datetime\"))).fillna({'lag_time': df.head().datetime})\n",
        "  timeFmt = \"yyyy-MM-dd' 'HH:mm:ss\"\n",
        "  timeDiff = (unix_timestamp('datetime', format=timeFmt) - unix_timestamp('lag_time', format=timeFmt))\n",
        "  df = df.withColumn(\"Duration\", timeDiff).drop('lag_time')\n",
        "  df = df.withColumn(\"Speed\", col(\"Dist\")/col(\"Duration\")).fillna({'Speed': 0.01})\n",
        "  \n",
        "  '''get_angle_udf = udf(lambda x,y,z,g: angle(x,y,z,g), FloatType())\n",
        "  df = df.withColumn(\"prev_X\", lag(\"X_coord\", 1).over(Window().orderBy(\"datetime\")))\n",
        "  df = df.withColumn(\"prev_Y\", lag(\"Y_coord\", 1).over(Window().orderBy(\"datetime\")))\n",
        "  df = df.withColumn(\"dx\", when(isnull(col('X_coord') - col('prev_X')), 0.01)\n",
        "                                .otherwise(col('X_coord') - col('prev_X'))).drop('prev_X')\n",
        "  df = df.withColumn(\"dy\", when(isnull(col('Y_coord') - col('prev_Y')), 0.01)\n",
        "                                .otherwise(col('Y_coord') - col('prev_Y'))).drop('prev_Y')\n",
        "  \n",
        "  df = df.withColumn(\"prev_dx\", lag(\"dx\", 1).over(Window().orderBy(\"datetime\")))\n",
        "  df = df.withColumn(\"prev_dy\", lag(\"dy\", 1).over(Window().orderBy(\"datetime\")))\n",
        "  df = df.withColumn(\"prev_dx\", when(isnull(col('prev_dx')), 0.01)\n",
        "                                .otherwise(col('prev_dx')))\n",
        "  df = df.withColumn(\"prev_dy\", when(isnull(col('prev_dy')), 0.01)\n",
        "                                .otherwise(col('prev_dy')))\n",
        "  \n",
        "  df = df.withColumn(\"Angle\", get_angle_udf(\"dx\", \"prev_dx\", \"dy\", \"prev_dy\")).drop('dx').drop('dy').drop('prev_dx').drop('prev_dy')\n",
        "  df = df.withColumn(\"DistByAngle\", when(isnull(col('Dist') / col('Angle')), 0.01)\n",
        "                                .otherwise(col('Dist') / col('Angle')))'''\n",
        "  \n",
        "  \n",
        "  return df \n",
        "\n",
        "\n",
        "\n",
        "temp = FeatureEng(df_expample)\n",
        "temp.show(50)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------+-------------------+-------------------+---------+---------+---------+--------+------------------+\n",
            "| Latitude|Longitude|Altitude|           datetime|Transportation Mode|  X_coord|  Y_coord|     Dist|Duration|             Speed|\n",
            "+---------+---------+--------+-------------------+-------------------+---------+---------+---------+--------+------------------+\n",
            "|41.765053| 83.34479|    -777|2008-04-03 16:00:00|              train| 550731.9|4719995.5|     0.01|       0|              0.01|\n",
            "|41.765114|83.345116|    -777|2008-04-03 16:00:01|              train| 550704.4|4719994.0|27.540878|       1|27.540878295898438|\n",
            "|41.765175| 83.34545|    -777|2008-04-03 16:00:02|              train| 550676.2|4719993.0|28.205233|       1|28.205232620239258|\n",
            "| 41.76523|83.345795|    -777|2008-04-03 16:00:03|              train|550647.44|4719992.5|28.754347|       1| 28.75434684753418|\n",
            "| 41.76528| 83.34614|    -777|2008-04-03 16:00:04|              train| 550618.7|4719991.5|28.767385|       1|28.767385482788086|\n",
            "|41.765343| 83.34647|    -777|2008-04-03 16:00:05|              train| 550590.5|4719990.5|28.205233|       1|28.205232620239258|\n",
            "|41.765404| 83.34681|    -777|2008-04-03 16:00:06|              train| 550562.3|4719989.0|28.227383|       1| 28.22738265991211|\n",
            "|41.765465|83.347145|    -777|2008-04-03 16:00:07|              train| 550534.1|4719988.0|28.205233|       1|28.205232620239258|\n",
            "|41.765522| 83.34749|    -777|2008-04-03 16:00:08|              train| 550505.4|4719987.0|28.767385|       1|28.767385482788086|\n",
            "| 41.76557| 83.34782|    -777|2008-04-03 16:00:09|              train|550477.94|4719986.5|27.442055|       1|27.442054748535156|\n",
            "| 41.76562| 83.34816|    -777|2008-04-03 16:00:10|              train|550449.25|4719986.0|28.691856|       1|28.691856384277344|\n",
            "|41.765675| 83.34851|    -777|2008-04-03 16:00:11|              train| 550419.9|4719985.5|29.379255|       1|29.379255294799805|\n",
            "|41.765728| 83.34885|    -777|2008-04-03 16:00:12|              train|550391.75|4719985.0|28.129444|       1|28.129444122314453|\n",
            "| 41.76578| 83.34918|    -777|2008-04-03 16:00:13|              train| 550363.6|4719984.0|28.142773|       1|28.142772674560547|\n",
            "|41.765842| 83.34951|    -777|2008-04-03 16:00:14|              train|550336.06|4719983.0|27.580635|       1| 27.58063507080078|\n",
            "|  41.7659|83.349846|    -777|2008-04-03 16:00:15|              train|550307.94|4719982.0|28.142773|       1|28.142772674560547|\n",
            "|41.765953|83.350174|    -777|2008-04-03 16:00:16|              train|550280.44|4719981.0|27.518175|       1| 27.51817512512207|\n",
            "|41.766006|83.350494|    -777|2008-04-03 16:00:17|              train|550253.56|4719980.5| 26.87965|       1|26.879650115966797|\n",
            "| 41.76607| 83.35083|    -777|2008-04-03 16:00:18|              train| 550225.4|4719978.5|28.258364|       1|28.258363723754883|\n",
            "| 41.76612| 83.35116|    -777|2008-04-03 16:00:19|              train|550197.94|4719978.0|27.442055|       1|27.442054748535156|\n",
            "| 41.76617| 83.35149|    -777|2008-04-03 16:00:20|              train| 550170.5|4719977.5|27.442055|       1|27.442054748535156|\n",
            "| 41.76623|83.351814|    -777|2008-04-03 16:00:21|              train|550142.94|4719976.5|27.580635|       1| 27.58063507080078|\n",
            "| 41.76628| 83.35214|    -777|2008-04-03 16:00:22|              train| 550115.5|4719976.0|27.442055|       1|27.442054748535156|\n",
            "|41.766327| 83.35247|    -777|2008-04-03 16:00:23|              train|550088.06|4719975.5|27.442055|       1|27.442054748535156|\n",
            "|41.766376|83.352806|    -777|2008-04-03 16:00:24|              train| 550060.0|4719975.0|28.066954|       1|28.066953659057617|\n",
            "|41.766415| 83.35313|    -777|2008-04-03 16:00:25|              train|550033.25|4719975.5|26.754673|       1| 26.75467300415039|\n",
            "| 41.76646| 83.35346|    -777|2008-04-03 16:00:26|              train| 550005.2|4719975.5|  28.0625|       1|           28.0625|\n",
            "|41.766506| 83.35378|    -777|2008-04-03 16:00:27|              train|549978.44|4719975.0|26.754673|       1| 26.75467300415039|\n",
            "|41.766556|  83.3541|    -777|2008-04-03 16:00:28|              train| 549951.6|4719974.5|26.817162|       1|26.817161560058594|\n",
            "|41.766613| 83.35444|    -777|2008-04-03 16:00:29|              train|549923.44|4719973.5|28.205233|       1|28.205232620239258|\n",
            "| 41.76669| 83.35478|    -777|2008-04-03 16:00:30|              train| 549894.5|4719971.0| 29.04529|       1|29.045289993286133|\n",
            "|41.766754|83.355125|    -777|2008-04-03 16:00:31|              train| 549865.7|4719969.5|28.851519|       1|28.851518630981445|\n",
            "|41.766815| 83.35546|    -777|2008-04-03 16:00:32|              train| 549837.5|4719968.5|28.205233|       1|28.205232620239258|\n",
            "| 41.76687|  83.3558|    -777|2008-04-03 16:00:33|              train| 549809.4|4719967.5|28.142773|       1|28.142772674560547|\n",
            "|41.766922| 83.35612|    -777|2008-04-03 16:00:34|              train|549782.56|4719967.0|26.817162|       1|26.817161560058594|\n",
            "| 41.76697|83.356445|    -777|2008-04-03 16:00:35|              train|549755.06|4719966.5|27.504545|       1|27.504545211791992|\n",
            "| 41.76702|83.356766|    -777|2008-04-03 16:00:36|              train|549728.25|4719966.0|26.817162|       1|26.817161560058594|\n",
            "| 41.76708|83.357086|    -777|2008-04-03 16:00:37|              train| 549701.4|4719964.5|26.916828|       1|26.916828155517578|\n",
            "|41.767143| 83.35741|    -777|2008-04-03 16:00:38|              train|549674.44|4719963.0| 26.97923|       1|26.979230880737305|\n",
            "|  41.7672| 83.35773|    -777|2008-04-03 16:00:39|              train|549647.56|4719962.0|26.893599|       1|26.893598556518555|\n",
            "| 41.76725| 83.35805|    -777|2008-04-03 16:00:40|              train| 549620.7|4719961.5| 26.87965|       1|26.879650115966797|\n",
            "| 41.76729|83.358376|    -777|2008-04-03 16:00:41|              train| 549593.4|4719961.5|  27.3125|       1|           27.3125|\n",
            "| 41.76734|  83.3587|    -777|2008-04-03 16:00:42|              train| 549565.9|4719961.0|27.504545|       1|27.504545211791992|\n",
            "|41.767403|83.359055|    -777|2008-04-03 16:00:43|              train|549536.44|4719959.5| 29.47569|       1|29.475690841674805|\n",
            "|41.767467|83.359406|    -777|2008-04-03 16:00:44|              train| 549507.0|4719958.5|29.454481|       1| 29.45448112487793|\n",
            "|41.767536| 83.35976|    -777|2008-04-03 16:00:45|              train| 549477.5|4719956.5|29.567719|       1|29.567718505859375|\n",
            "|41.767612|  83.3601|    -777|2008-04-03 16:00:46|              train|549448.56|4719954.5|29.006533|       1|29.006532669067383|\n",
            "| 41.76769| 83.36044|    -777|2008-04-03 16:00:47|              train| 549419.6|4719952.0| 29.04529|       1|29.045289993286133|\n",
            "| 41.76776| 83.36076|    -777|2008-04-03 16:00:48|              train|549392.56|4719949.5|27.177729|       1|  27.1777286529541|\n",
            "|41.767834|  83.3611|    -777|2008-04-03 16:00:49|              train| 549364.3|4719947.5|28.320707|       1|28.320707321166992|\n",
            "+---------+---------+--------+-------------------+-------------------+---------+---------+---------+--------+------------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2LbE0dN5Ih"
      },
      "source": [
        "**Теперь переходим в генерации окон.** При помощи их, можно будет создать ранговые и аггрегационные функции наших исходных фичей в заданном промежутке времени. Экспериментальным путем было решено использовать размер окна **120 секунд**, так как есть корокие логи, которые даже не захватили б больший промежуток. А там хранилась полезная информаци про логирование пути в метро (лог попросту прерывался и не был продолжительным по времени).\n",
        "\n",
        "И параллельно проведем **Label Encoding** наших меток таргета, т.е. заменим метки класса на цифровые значения и обьеденим несколько классов в один, так как по своей логике перемещения они ничем не отличаются и отличить их будет сложно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dd3e731e-488b-4f75-81b1-d61455058607"
        },
        "id": "uNKBZJZcrS4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b5da08-3a8f-4ddf-debd-d1194ee836e6"
      },
      "source": [
        "def CreateWindow(df):\n",
        "  \n",
        "  agg_temp = df.groupBy(window(df.datetime, \"120 seconds\")).agg(mean('X_coord'), \\\n",
        "                                                                mean('Y_coord'), \\\n",
        "                                                                mean('Altitude'), \\\n",
        "                                                                #mean('Dist'), \\\n",
        "                                                                sum('Dist'), \\\n",
        "                                                                stddev('Dist'), \\\n",
        "                                                                mean('Speed'), \\\n",
        "                                                                #first('Speed'), \\\n",
        "                                                                (-first('Speed')+last('Speed')).alias('DeltaSpeed'), \\\n",
        "                                                                #mean('Angle'), \\\n",
        "                                                                #stddev('Angle'), \\\n",
        "                                                                #mean('DistByAngle'), \\\n",
        "                                                                first('Transportation Mode')).drop('window')\n",
        "  TransportDict = {'walk':1,\n",
        "              'bike':2,\n",
        "              'bus':3,\n",
        "              'car':4,\n",
        "              'taxi':4,\n",
        "              'subway,':5,\n",
        "              'train':6,\n",
        "              'airplane':7,\n",
        "              'boat':8,\n",
        "              'motorcycle':9}\n",
        "\n",
        "  mapping = create_map([lit(x) for x in chain(*TransportDict.items())])\n",
        "  agg_temp = agg_temp.withColumn('Transportation Mode', mapping[agg_temp['first(Transportation Mode)']]).drop('first(Transportation Mode)')\n",
        "  \n",
        "  return agg_temp\n",
        "\n",
        "temp_final = CreateWindow(temp)\n",
        "temp_final.show(1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+-------------+------------------+-----------------+------------------+------------------+-------------------+\n",
            "|     avg(X_coord)|     avg(Y_coord)|avg(Altitude)|         sum(Dist)|stddev_samp(Dist)|        avg(Speed)|        DeltaSpeed|Transportation Mode|\n",
            "+-----------------+-----------------+-------------+------------------+-----------------+------------------+------------------+-------------------+\n",
            "|549080.6384698276|4719942.254310345|       -777.0|3373.7341477964073|8.125190130579002|28.090952043642947|30.379400482177733|                  6|\n",
            "+-----------------+-----------------+-------------+------------------+-----------------+------------------+------------------+-------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xXzkzwyPXN8"
      },
      "source": [
        "## Генерализация функций\n",
        "\n",
        "Переносим наши полученные фичи на всех юзеров. Для этого проходим по всем юзерам и логам и проводим предобработку каждого отдельно. **Альтернативный подход:** локально соединить все логи с указанием юзера, но могли возникнуть вопросы с накладыванием данных одного юзера на другого в пределах одного окна + для допольнительного задания было б сложно добавлять в нужное место новую потоковую информацию про юзера."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Оборачиваем в функцию",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "d4ac6a71-77dc-4159-bebe-3bfde4b48082"
        },
        "id": "yqLHxBWjrS4q"
      },
      "source": [
        "import azure.storage\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "import os\n",
        "\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=mlstorageall;AccountKey=djvx9juh6EJZWqx7T3U2D61BwhPWyEx4BcVyzbt/AQhaNR8hGmVBtzaJnbbG2w4c67p4dZkZIAWK6kq1PbV8Zg==;EndpointSuffix=core.windows.net\"\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "container_client=blob_service_client.get_container_client(blob_container)\n",
        "\n",
        "user = None\n",
        "trajs = []\n",
        "usersDict = {}\n",
        "blob_list = container_client.list_blobs(name_starts_with=\"AllData_preprocess/\")\n",
        "\n",
        "for blob in blob_list:\n",
        "  path_list = blob.name.split(os.sep)\n",
        "  if path_list[1]!=user:\n",
        "    user = path_list[1]\n",
        "    trajs = []\n",
        "  if len(path_list)>3:\n",
        "    if (path_list[2]!='labels_csv.csv') & (path_list[1]==user):\n",
        "      trajs.append(path_list[3])\n",
        "      usersDict[user] = trajs\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieYoNSutQcv_"
      },
      "source": [
        "Получаем итоговый датафрейм с аггрегированными фичами для всех юзеров и метками таргета. Ориентировочное время работы 4 часа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ec26c62d-fd52-4416-a15c-4e99e245d41e"
        },
        "id": "KZ6Vqq62rS4r"
      },
      "source": [
        "aggschema = StructType([ \\\n",
        "      StructField(\"avg(X_coord)\", FloatType(), True), \\\n",
        "      StructField(\"avg(Y_coord)\", FloatType(), True), \\\n",
        "      StructField(\"avg(Altitude)\", FloatType(), True), \\\n",
        "      #StructField(\"avg(Dist)\", FloatType(), True), \\\n",
        "      StructField(\"sum(Dist)\", FloatType(), True), \\\n",
        "      StructField(\"stddev_samp(Dist)\", FloatType(), True), \\\n",
        "      StructField(\"avg(Speed)\", FloatType(), True), \\\n",
        "      #StructField(\"first(Speed)\", FloatType(), True), \\\n",
        "      StructField(\"DeltaSpeed\", FloatType(), True), \\\n",
        "      #StructField(\"avg(Angle)\", FloatType(), True), \\\n",
        "      #StructField(\"stddev_samp(Angle)\", FloatType(), True), \\\n",
        "      #StructField(\"avg(DistByAngle)\", FloatType(), True), \\\n",
        "      StructField(\"Transportation Mode\", IntegerType(), True)])\n",
        "\n",
        "df_full = spark.createDataFrame(data=[], schema=aggschema)\n",
        "for user in usersDict:\n",
        "  print(user)\n",
        "  for log in usersDict[user]:\n",
        "    #print(log)\n",
        "    df = readcsv(\"/\" + user + \"/Traj_Labelled/\" + log, geoSchema)\n",
        "    df1 = FeatureEng(df)\n",
        "    df2 = CreateWindow(df1)\n",
        "    df_full = df_full.unionByName(df2)\n",
        "  #print(user)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH30G24rQmIP"
      },
      "source": [
        "## Моделинг\n",
        "\n",
        "Я должен выполнить некоторые преобразования, чтобы объединить все столбцы функций в один столбец при помощи **VectorAssembler** для дальнейшего моделирования. Заменяем столбцы функций одним столбом с вектором функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrGoBzk44078"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "df_full_without_nan = df_full.fillna(0)\n",
        "vector_assembler = VectorAssembler(\\\n",
        "inputCols=[\"avg(X_coord)\", \"avg(Y_coord)\", \"avg(Altitude)\", \"sum(Dist)\", \"stddev_samp(Dist)\", \"avg(Speed)\", \"DeltaSpeed\"],\\\n",
        "outputCol=\"features\")\n",
        "df_temp = vector_assembler.transform(df_full_without_nan)\n",
        "df = df_temp.drop(\"avg(X_coord)\", \"avg(Y_coord)\", \"avg(Altitude)\", \"sum(Dist)\", \"stddev_samp(Dist)\", \"avg(Speed)\", \"DeltaSpeed\")\n",
        "#df.show(5)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaG-tKdfRg31"
      },
      "source": [
        "После векторизации нужно разделить наши данные на обучающие и тестовые наборы (30% оставлено для тестирования):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlJq9ojd6fk2"
      },
      "source": [
        "(train, test) = df.randomSplit([0.70,0.30])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhTuHLfrRrGn"
      },
      "source": [
        "В качестве модели используем **RandomForest**. Ансамбли деревьев решений являются одними из самых популярных алгоритмов для задач классификации. Из-за комбинации множества деревьев решений классификатор случайного леса имеет меньший риск переобучения. API на основе DataFrame для машинного обучения поддерживает случайные леса как для двоичной, так и для мультиклассовой классификации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1HXGaud69bs"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"Transportation Mode\", featuresCol=\"features\", numTrees=10)\n",
        "dt = DecisionTreeClassifier(labelCol=\"Transportation Mode\", featuresCol=\"features\")\n",
        "model = rf.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0KIP4JQR8ZO"
      },
      "source": [
        "Теперь можно делать прогнозы и смотреть метрики:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya2z7tGkeylX"
      },
      "source": [
        "predictions = model.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_LvtnKAe24g",
        "outputId": "9caae1a0-5b4d-42b5-a3cb-8498496e05fd"
      },
      "source": [
        "predictions.select(\"prediction\", \"Transportation Mode\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------------+\n",
            "|prediction|Transportation Mode|\n",
            "+----------+-------------------+\n",
            "|       6.0|                  4|\n",
            "|       1.0|                  1|\n",
            "|       1.0|                  1|\n",
            "|       1.0|                  1|\n",
            "|       1.0|                  1|\n",
            "+----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C07mZmCXSFPK"
      },
      "source": [
        "Чтобы оценить точность прогноза, необходимо вычислить точность на тесте:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xbVn7KAfEjx",
        "outputId": "f470982f-00bf-4007-faee-85f614d6d893"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator =\\\n",
        "MulticlassClassificationEvaluator(labelCol=\"Transportation Mode\",\\\n",
        "predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test set accuracy = \", accuracy)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy =  0.7026258205689278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgDUup3dSMuX"
      },
      "source": [
        "Точность достаточно неплохая, учитывая количество классов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdlgjRanSmMh",
        "outputId": "ceec5268-8ee8-4894-c8d4-a08be83e6d2a"
      },
      "source": [
        "!pip install nbconvert"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (5.6.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.11.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.2.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (5.0.8)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.3.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.7.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (20.4)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (4.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWcxKIdSwIR",
        "outputId": "8d94116f-9ed5-4ee1-a0c5-754db0886732"
      },
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/Spark_GeoLife_HW_Samoshyn.ipynb"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NbConvertApp] Converting notebook /content/Spark_GeoLife_HW_Samoshyn.ipynb to html\n",
            "[NbConvertApp] Writing 730266 bytes to /content/Spark_GeoLife_HW_Samoshyn.html\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}